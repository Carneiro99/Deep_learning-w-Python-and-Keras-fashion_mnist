{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning w/  Keras and fashion_mnist.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXfHZWpHCiK7",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq3zfgx9WFBX",
        "colab_type": "code",
        "outputId": "d1d346dc-6ecc-4a39-eaf9-61aeaa785894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "#imports\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VkEboZcCfPe",
        "colab_type": "text"
      },
      "source": [
        "## Carregando o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGIzBa53_AEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = keras.datasets.fashion_mnist\n",
        "((imagens_treino, id_treino), (imagens_teste, id_teste)) = dataset.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhGwz_SzCry8",
        "colab_type": "text"
      },
      "source": [
        "## Entendendo o Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNZtbMna-_6_",
        "colab_type": "code",
        "outputId": "28dc51b6-146e-424f-ebf1-23b338b886df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\n",
        "len(imagens_treino)\n",
        "imagens_treino.shape\n",
        "imagens_teste.shape\n",
        "len(id_teste)\n",
        "id_treino.min()\n",
        "id_treino.max()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XKjiKkWCvP-",
        "colab_type": "text"
      },
      "source": [
        "## Exibindo os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImFD9__LWZ-i",
        "colab_type": "code",
        "outputId": "2f1b3654-a6ee-4a7c-e5c1-b3680f88ecea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "\n",
        "total_classificacoes = 10\n",
        "nomes_classificacoes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress',\n",
        "'Coat', 'Sandal','Shirt', 'Sneaker','Bag' ,'Ankle boot' ]\n",
        "'''\n",
        "plt.imshow(imagens_treino[0])\n",
        "plt.title(id_treino[0])\n",
        "\n",
        "\n",
        "for imagem in range (10):\n",
        "  plt.subplot(2, 5 , imagem +1) #2 linha e 5 colunas\n",
        "  plt.imshow(imagens_treino[imagem])\n",
        "  plt.title(nomes_classificacoes[id_treino[imagem]])\n",
        "'''\n",
        "plt.imshow(imagens_treino[0])\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.colorbar.Colorbar at 0x7f899232deb8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSmIiUmhKYiJSaEpiIlJoSmIiUmj/H4BqExLuMX2fAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG7BcX2sXbKN",
        "colab_type": "text"
      },
      "source": [
        "## Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIA5jSKgxnRN",
        "colab_type": "code",
        "outputId": "0da7925c-cf10-4dbd-fe9c-e8277cb544db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#normalização\n",
        "imagens_treino = imagens_treino/float(255)\n",
        "\n",
        "modelo = keras.Sequential([\n",
        "     #entrada\n",
        "     keras.layers.Flatten(input_shape=(28, 28)),\n",
        "     \n",
        "     #processamento\n",
        "     keras.layers.Dense(258, activation=tensorflow.nn.relu), #relu: aprendizagem nao linear\n",
        "    \n",
        "\n",
        "     #saida\n",
        "     keras.layers.Dense(10, activation=tensorflow.nn.softmax) \n",
        "     ])\n",
        "\n",
        "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy',\n",
        "               metrics=['accuracy'])\n",
        "historico = modelo.fit(imagens_treino, id_treino, epochs=1000, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 1.0514 - acc: 0.6635 - val_loss: 0.6744 - val_acc: 0.7570\n",
            "Epoch 2/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.6216 - acc: 0.7762 - val_loss: 0.5760 - val_acc: 0.7928\n",
            "Epoch 3/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.5475 - acc: 0.8056 - val_loss: 0.5241 - val_acc: 0.8149\n",
            "Epoch 4/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.5066 - acc: 0.8198 - val_loss: 0.4952 - val_acc: 0.8241\n",
            "Epoch 5/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.4798 - acc: 0.8311 - val_loss: 0.4744 - val_acc: 0.8343\n",
            "Epoch 6/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.4613 - acc: 0.8386 - val_loss: 0.4590 - val_acc: 0.8367\n",
            "Epoch 7/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.4466 - acc: 0.8426 - val_loss: 0.4494 - val_acc: 0.8384\n",
            "Epoch 8/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.4353 - acc: 0.8456 - val_loss: 0.4363 - val_acc: 0.8460\n",
            "Epoch 9/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.4251 - acc: 0.8495 - val_loss: 0.4310 - val_acc: 0.8461\n",
            "Epoch 10/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.4168 - acc: 0.8527 - val_loss: 0.4235 - val_acc: 0.8478\n",
            "Epoch 11/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.4093 - acc: 0.8563 - val_loss: 0.4178 - val_acc: 0.8491\n",
            "Epoch 12/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.4026 - acc: 0.8589 - val_loss: 0.4175 - val_acc: 0.8519\n",
            "Epoch 13/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.3970 - acc: 0.8600 - val_loss: 0.4086 - val_acc: 0.8543\n",
            "Epoch 14/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3909 - acc: 0.8615 - val_loss: 0.4059 - val_acc: 0.8542\n",
            "Epoch 15/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.3863 - acc: 0.8636 - val_loss: 0.4048 - val_acc: 0.8564\n",
            "Epoch 16/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3809 - acc: 0.8648 - val_loss: 0.3972 - val_acc: 0.8587\n",
            "Epoch 17/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3770 - acc: 0.8664 - val_loss: 0.3950 - val_acc: 0.8596\n",
            "Epoch 18/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.3722 - acc: 0.8680 - val_loss: 0.3923 - val_acc: 0.8600\n",
            "Epoch 19/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3685 - acc: 0.8697 - val_loss: 0.3899 - val_acc: 0.8633\n",
            "Epoch 20/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3647 - acc: 0.8707 - val_loss: 0.3890 - val_acc: 0.8602\n",
            "Epoch 21/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3611 - acc: 0.8720 - val_loss: 0.3793 - val_acc: 0.8652\n",
            "Epoch 22/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3570 - acc: 0.8741 - val_loss: 0.3830 - val_acc: 0.8624\n",
            "Epoch 23/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3532 - acc: 0.8743 - val_loss: 0.3771 - val_acc: 0.8654\n",
            "Epoch 24/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3508 - acc: 0.8743 - val_loss: 0.3734 - val_acc: 0.8684\n",
            "Epoch 25/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3472 - acc: 0.8766 - val_loss: 0.3724 - val_acc: 0.8683\n",
            "Epoch 26/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3436 - acc: 0.8786 - val_loss: 0.3779 - val_acc: 0.8639\n",
            "Epoch 27/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3401 - acc: 0.8780 - val_loss: 0.3666 - val_acc: 0.8690\n",
            "Epoch 28/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3372 - acc: 0.8800 - val_loss: 0.3680 - val_acc: 0.8702\n",
            "Epoch 29/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3342 - acc: 0.8811 - val_loss: 0.3668 - val_acc: 0.8694\n",
            "Epoch 30/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3308 - acc: 0.8824 - val_loss: 0.3594 - val_acc: 0.8724\n",
            "Epoch 31/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3285 - acc: 0.8823 - val_loss: 0.3583 - val_acc: 0.8726\n",
            "Epoch 32/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3261 - acc: 0.8848 - val_loss: 0.3606 - val_acc: 0.8721\n",
            "Epoch 33/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3234 - acc: 0.8850 - val_loss: 0.3556 - val_acc: 0.8733\n",
            "Epoch 34/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3204 - acc: 0.8857 - val_loss: 0.3568 - val_acc: 0.8737\n",
            "Epoch 35/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.3179 - acc: 0.8871 - val_loss: 0.3544 - val_acc: 0.8746\n",
            "Epoch 36/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.3148 - acc: 0.8875 - val_loss: 0.3569 - val_acc: 0.8727\n",
            "Epoch 37/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3130 - acc: 0.8882 - val_loss: 0.3542 - val_acc: 0.8731\n",
            "Epoch 38/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.3104 - acc: 0.8890 - val_loss: 0.3530 - val_acc: 0.8740\n",
            "Epoch 39/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.3080 - acc: 0.8891 - val_loss: 0.3500 - val_acc: 0.8762\n",
            "Epoch 40/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.3055 - acc: 0.8913 - val_loss: 0.3508 - val_acc: 0.8747\n",
            "Epoch 41/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.3034 - acc: 0.8910 - val_loss: 0.3438 - val_acc: 0.8782\n",
            "Epoch 42/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.3011 - acc: 0.8918 - val_loss: 0.3432 - val_acc: 0.8783\n",
            "Epoch 43/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2987 - acc: 0.8926 - val_loss: 0.3480 - val_acc: 0.8723\n",
            "Epoch 44/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2960 - acc: 0.8938 - val_loss: 0.3453 - val_acc: 0.8776\n",
            "Epoch 45/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2945 - acc: 0.8945 - val_loss: 0.3485 - val_acc: 0.8740\n",
            "Epoch 46/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2927 - acc: 0.8940 - val_loss: 0.3371 - val_acc: 0.8794\n",
            "Epoch 47/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2905 - acc: 0.8954 - val_loss: 0.3392 - val_acc: 0.8785\n",
            "Epoch 48/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2878 - acc: 0.8959 - val_loss: 0.3422 - val_acc: 0.8777\n",
            "Epoch 49/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2865 - acc: 0.8969 - val_loss: 0.3384 - val_acc: 0.8802\n",
            "Epoch 50/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2844 - acc: 0.8976 - val_loss: 0.3337 - val_acc: 0.8808\n",
            "Epoch 51/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2827 - acc: 0.8981 - val_loss: 0.3316 - val_acc: 0.8808\n",
            "Epoch 52/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2805 - acc: 0.8987 - val_loss: 0.3311 - val_acc: 0.8827\n",
            "Epoch 53/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2783 - acc: 0.8992 - val_loss: 0.3344 - val_acc: 0.8802\n",
            "Epoch 54/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2768 - acc: 0.9016 - val_loss: 0.3324 - val_acc: 0.8804\n",
            "Epoch 55/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2753 - acc: 0.9006 - val_loss: 0.3312 - val_acc: 0.8842\n",
            "Epoch 56/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2730 - acc: 0.9022 - val_loss: 0.3298 - val_acc: 0.8832\n",
            "Epoch 57/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.2713 - acc: 0.9025 - val_loss: 0.3269 - val_acc: 0.8836\n",
            "Epoch 58/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2691 - acc: 0.9039 - val_loss: 0.3301 - val_acc: 0.8809\n",
            "Epoch 59/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2680 - acc: 0.9034 - val_loss: 0.3314 - val_acc: 0.8815\n",
            "Epoch 60/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2659 - acc: 0.9049 - val_loss: 0.3270 - val_acc: 0.8849\n",
            "Epoch 61/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2641 - acc: 0.9052 - val_loss: 0.3312 - val_acc: 0.8823\n",
            "Epoch 62/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2631 - acc: 0.9053 - val_loss: 0.3310 - val_acc: 0.8795\n",
            "Epoch 63/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2612 - acc: 0.9064 - val_loss: 0.3235 - val_acc: 0.8852\n",
            "Epoch 64/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2601 - acc: 0.9064 - val_loss: 0.3248 - val_acc: 0.8840\n",
            "Epoch 65/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2581 - acc: 0.9068 - val_loss: 0.3380 - val_acc: 0.8835\n",
            "Epoch 66/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2561 - acc: 0.9078 - val_loss: 0.3333 - val_acc: 0.8832\n",
            "Epoch 67/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2550 - acc: 0.9086 - val_loss: 0.3239 - val_acc: 0.8853\n",
            "Epoch 68/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2539 - acc: 0.9080 - val_loss: 0.3224 - val_acc: 0.8859\n",
            "Epoch 69/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2518 - acc: 0.9101 - val_loss: 0.3250 - val_acc: 0.8838\n",
            "Epoch 70/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2503 - acc: 0.9094 - val_loss: 0.3205 - val_acc: 0.8864\n",
            "Epoch 71/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2491 - acc: 0.9101 - val_loss: 0.3221 - val_acc: 0.8856\n",
            "Epoch 72/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.2478 - acc: 0.9111 - val_loss: 0.3219 - val_acc: 0.8830\n",
            "Epoch 73/1000\n",
            "48000/48000 [==============================] - 5s 106us/sample - loss: 0.2464 - acc: 0.9112 - val_loss: 0.3219 - val_acc: 0.8856\n",
            "Epoch 74/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.2444 - acc: 0.9126 - val_loss: 0.3229 - val_acc: 0.8842\n",
            "Epoch 75/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2434 - acc: 0.9126 - val_loss: 0.3222 - val_acc: 0.8849\n",
            "Epoch 76/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2422 - acc: 0.9136 - val_loss: 0.3192 - val_acc: 0.8870\n",
            "Epoch 77/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2405 - acc: 0.9135 - val_loss: 0.3187 - val_acc: 0.8867\n",
            "Epoch 78/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2387 - acc: 0.9148 - val_loss: 0.3234 - val_acc: 0.8854\n",
            "Epoch 79/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2379 - acc: 0.9135 - val_loss: 0.3168 - val_acc: 0.8860\n",
            "Epoch 80/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2362 - acc: 0.9153 - val_loss: 0.3180 - val_acc: 0.8883\n",
            "Epoch 81/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2352 - acc: 0.9155 - val_loss: 0.3188 - val_acc: 0.8888\n",
            "Epoch 82/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.2343 - acc: 0.9162 - val_loss: 0.3213 - val_acc: 0.8845\n",
            "Epoch 83/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2324 - acc: 0.9165 - val_loss: 0.3160 - val_acc: 0.8878\n",
            "Epoch 84/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2311 - acc: 0.9167 - val_loss: 0.3187 - val_acc: 0.8867\n",
            "Epoch 85/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2297 - acc: 0.9178 - val_loss: 0.3212 - val_acc: 0.8878\n",
            "Epoch 86/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.2286 - acc: 0.9177 - val_loss: 0.3157 - val_acc: 0.8861\n",
            "Epoch 87/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2282 - acc: 0.9179 - val_loss: 0.3130 - val_acc: 0.8888\n",
            "Epoch 88/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2263 - acc: 0.9192 - val_loss: 0.3179 - val_acc: 0.8881\n",
            "Epoch 89/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2252 - acc: 0.9193 - val_loss: 0.3193 - val_acc: 0.8894\n",
            "Epoch 90/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2239 - acc: 0.9192 - val_loss: 0.3271 - val_acc: 0.8854\n",
            "Epoch 91/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2237 - acc: 0.9195 - val_loss: 0.3202 - val_acc: 0.8856\n",
            "Epoch 92/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2216 - acc: 0.9210 - val_loss: 0.3145 - val_acc: 0.8888\n",
            "Epoch 93/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2207 - acc: 0.9212 - val_loss: 0.3147 - val_acc: 0.8882\n",
            "Epoch 94/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2193 - acc: 0.9223 - val_loss: 0.3176 - val_acc: 0.8885\n",
            "Epoch 95/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2185 - acc: 0.9211 - val_loss: 0.3202 - val_acc: 0.8867\n",
            "Epoch 96/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2175 - acc: 0.9217 - val_loss: 0.3139 - val_acc: 0.8892\n",
            "Epoch 97/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2156 - acc: 0.9225 - val_loss: 0.3180 - val_acc: 0.8894\n",
            "Epoch 98/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2154 - acc: 0.9235 - val_loss: 0.3174 - val_acc: 0.8897\n",
            "Epoch 99/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2144 - acc: 0.9231 - val_loss: 0.3164 - val_acc: 0.8878\n",
            "Epoch 100/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.2129 - acc: 0.9249 - val_loss: 0.3169 - val_acc: 0.8899\n",
            "Epoch 101/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.2116 - acc: 0.9251 - val_loss: 0.3228 - val_acc: 0.8868\n",
            "Epoch 102/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.2102 - acc: 0.9248 - val_loss: 0.3158 - val_acc: 0.8877\n",
            "Epoch 103/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2100 - acc: 0.9252 - val_loss: 0.3161 - val_acc: 0.8881\n",
            "Epoch 104/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2085 - acc: 0.9255 - val_loss: 0.3151 - val_acc: 0.8903\n",
            "Epoch 105/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2080 - acc: 0.9254 - val_loss: 0.3211 - val_acc: 0.8877\n",
            "Epoch 106/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.2063 - acc: 0.9273 - val_loss: 0.3190 - val_acc: 0.8882\n",
            "Epoch 107/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.2060 - acc: 0.9269 - val_loss: 0.3274 - val_acc: 0.8857\n",
            "Epoch 108/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2047 - acc: 0.9271 - val_loss: 0.3154 - val_acc: 0.8880\n",
            "Epoch 109/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.2036 - acc: 0.9276 - val_loss: 0.3331 - val_acc: 0.8859\n",
            "Epoch 110/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2022 - acc: 0.9282 - val_loss: 0.3158 - val_acc: 0.8913\n",
            "Epoch 111/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2014 - acc: 0.9284 - val_loss: 0.3147 - val_acc: 0.8901\n",
            "Epoch 112/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.2005 - acc: 0.9286 - val_loss: 0.3195 - val_acc: 0.8887\n",
            "Epoch 113/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1988 - acc: 0.9292 - val_loss: 0.3189 - val_acc: 0.8879\n",
            "Epoch 114/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1992 - acc: 0.9291 - val_loss: 0.3297 - val_acc: 0.8817\n",
            "Epoch 115/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1985 - acc: 0.9287 - val_loss: 0.3148 - val_acc: 0.8895\n",
            "Epoch 116/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1972 - acc: 0.9293 - val_loss: 0.3176 - val_acc: 0.8892\n",
            "Epoch 117/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1957 - acc: 0.9315 - val_loss: 0.3150 - val_acc: 0.8897\n",
            "Epoch 118/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1944 - acc: 0.9311 - val_loss: 0.3160 - val_acc: 0.8898\n",
            "Epoch 119/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1934 - acc: 0.9312 - val_loss: 0.3165 - val_acc: 0.8899\n",
            "Epoch 120/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.1924 - acc: 0.9315 - val_loss: 0.3160 - val_acc: 0.8893\n",
            "Epoch 121/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1920 - acc: 0.9317 - val_loss: 0.3178 - val_acc: 0.8885\n",
            "Epoch 122/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1915 - acc: 0.9317 - val_loss: 0.3192 - val_acc: 0.8879\n",
            "Epoch 123/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1900 - acc: 0.9323 - val_loss: 0.3317 - val_acc: 0.8867\n",
            "Epoch 124/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1899 - acc: 0.9324 - val_loss: 0.3210 - val_acc: 0.8875\n",
            "Epoch 125/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1880 - acc: 0.9335 - val_loss: 0.3240 - val_acc: 0.8878\n",
            "Epoch 126/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1877 - acc: 0.9335 - val_loss: 0.3180 - val_acc: 0.8903\n",
            "Epoch 127/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1869 - acc: 0.9340 - val_loss: 0.3244 - val_acc: 0.8871\n",
            "Epoch 128/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1865 - acc: 0.9326 - val_loss: 0.3196 - val_acc: 0.8898\n",
            "Epoch 129/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1848 - acc: 0.9344 - val_loss: 0.3270 - val_acc: 0.8879\n",
            "Epoch 130/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1836 - acc: 0.9345 - val_loss: 0.3226 - val_acc: 0.8873\n",
            "Epoch 131/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1833 - acc: 0.9349 - val_loss: 0.3216 - val_acc: 0.8900\n",
            "Epoch 132/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1821 - acc: 0.9357 - val_loss: 0.3282 - val_acc: 0.8864\n",
            "Epoch 133/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1813 - acc: 0.9359 - val_loss: 0.3185 - val_acc: 0.8906\n",
            "Epoch 134/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1805 - acc: 0.9367 - val_loss: 0.3269 - val_acc: 0.8876\n",
            "Epoch 135/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1794 - acc: 0.9371 - val_loss: 0.3244 - val_acc: 0.8881\n",
            "Epoch 136/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1791 - acc: 0.9367 - val_loss: 0.3212 - val_acc: 0.8894\n",
            "Epoch 137/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1788 - acc: 0.9379 - val_loss: 0.3226 - val_acc: 0.8878\n",
            "Epoch 138/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1771 - acc: 0.9371 - val_loss: 0.3215 - val_acc: 0.8905\n",
            "Epoch 139/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1770 - acc: 0.9368 - val_loss: 0.3273 - val_acc: 0.8870\n",
            "Epoch 140/1000\n",
            "48000/48000 [==============================] - 5s 104us/sample - loss: 0.1759 - acc: 0.9379 - val_loss: 0.3185 - val_acc: 0.8911\n",
            "Epoch 141/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1754 - acc: 0.9381 - val_loss: 0.3201 - val_acc: 0.8905\n",
            "Epoch 142/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1745 - acc: 0.9389 - val_loss: 0.3199 - val_acc: 0.8904\n",
            "Epoch 143/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1727 - acc: 0.9389 - val_loss: 0.3251 - val_acc: 0.8882\n",
            "Epoch 144/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1723 - acc: 0.9397 - val_loss: 0.3206 - val_acc: 0.8898\n",
            "Epoch 145/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1720 - acc: 0.9402 - val_loss: 0.3235 - val_acc: 0.8902\n",
            "Epoch 146/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1708 - acc: 0.9408 - val_loss: 0.3230 - val_acc: 0.8897\n",
            "Epoch 147/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1702 - acc: 0.9403 - val_loss: 0.3272 - val_acc: 0.8885\n",
            "Epoch 148/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1693 - acc: 0.9408 - val_loss: 0.3235 - val_acc: 0.8904\n",
            "Epoch 149/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1684 - acc: 0.9406 - val_loss: 0.3284 - val_acc: 0.8875\n",
            "Epoch 150/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1684 - acc: 0.9406 - val_loss: 0.3279 - val_acc: 0.8901\n",
            "Epoch 151/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1678 - acc: 0.9410 - val_loss: 0.3296 - val_acc: 0.8893\n",
            "Epoch 152/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1670 - acc: 0.9409 - val_loss: 0.3270 - val_acc: 0.8883\n",
            "Epoch 153/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1656 - acc: 0.9419 - val_loss: 0.3341 - val_acc: 0.8885\n",
            "Epoch 154/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1652 - acc: 0.9428 - val_loss: 0.3286 - val_acc: 0.8885\n",
            "Epoch 155/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1640 - acc: 0.9423 - val_loss: 0.3309 - val_acc: 0.8889\n",
            "Epoch 156/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1633 - acc: 0.9425 - val_loss: 0.3282 - val_acc: 0.8900\n",
            "Epoch 157/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1631 - acc: 0.9430 - val_loss: 0.3306 - val_acc: 0.8898\n",
            "Epoch 158/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1618 - acc: 0.9431 - val_loss: 0.3269 - val_acc: 0.8892\n",
            "Epoch 159/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1610 - acc: 0.9436 - val_loss: 0.3289 - val_acc: 0.8901\n",
            "Epoch 160/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1608 - acc: 0.9445 - val_loss: 0.3332 - val_acc: 0.8901\n",
            "Epoch 161/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1600 - acc: 0.9442 - val_loss: 0.3275 - val_acc: 0.8903\n",
            "Epoch 162/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1591 - acc: 0.9440 - val_loss: 0.3338 - val_acc: 0.8897\n",
            "Epoch 163/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1587 - acc: 0.9443 - val_loss: 0.3308 - val_acc: 0.8915\n",
            "Epoch 164/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1581 - acc: 0.9453 - val_loss: 0.3321 - val_acc: 0.8907\n",
            "Epoch 165/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1577 - acc: 0.9445 - val_loss: 0.3373 - val_acc: 0.8895\n",
            "Epoch 166/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1559 - acc: 0.9456 - val_loss: 0.3353 - val_acc: 0.8913\n",
            "Epoch 167/1000\n",
            "48000/48000 [==============================] - 5s 104us/sample - loss: 0.1556 - acc: 0.9456 - val_loss: 0.3312 - val_acc: 0.8910\n",
            "Epoch 168/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1547 - acc: 0.9462 - val_loss: 0.3310 - val_acc: 0.8898\n",
            "Epoch 169/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1546 - acc: 0.9467 - val_loss: 0.3364 - val_acc: 0.8875\n",
            "Epoch 170/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1537 - acc: 0.9469 - val_loss: 0.3320 - val_acc: 0.8913\n",
            "Epoch 171/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1529 - acc: 0.9471 - val_loss: 0.3340 - val_acc: 0.8894\n",
            "Epoch 172/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1521 - acc: 0.9464 - val_loss: 0.3357 - val_acc: 0.8891\n",
            "Epoch 173/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1521 - acc: 0.9476 - val_loss: 0.3311 - val_acc: 0.8908\n",
            "Epoch 174/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1514 - acc: 0.9478 - val_loss: 0.3351 - val_acc: 0.8914\n",
            "Epoch 175/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1505 - acc: 0.9475 - val_loss: 0.3355 - val_acc: 0.8898\n",
            "Epoch 176/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1497 - acc: 0.9477 - val_loss: 0.3370 - val_acc: 0.8884\n",
            "Epoch 177/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1492 - acc: 0.9484 - val_loss: 0.3350 - val_acc: 0.8886\n",
            "Epoch 178/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1483 - acc: 0.9486 - val_loss: 0.3375 - val_acc: 0.8905\n",
            "Epoch 179/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1479 - acc: 0.9493 - val_loss: 0.3477 - val_acc: 0.8860\n",
            "Epoch 180/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1475 - acc: 0.9493 - val_loss: 0.3394 - val_acc: 0.8894\n",
            "Epoch 181/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1463 - acc: 0.9499 - val_loss: 0.3414 - val_acc: 0.8895\n",
            "Epoch 182/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1460 - acc: 0.9503 - val_loss: 0.3377 - val_acc: 0.8902\n",
            "Epoch 183/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1455 - acc: 0.9501 - val_loss: 0.3508 - val_acc: 0.8871\n",
            "Epoch 184/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1450 - acc: 0.9491 - val_loss: 0.3470 - val_acc: 0.8903\n",
            "Epoch 185/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1446 - acc: 0.9501 - val_loss: 0.3391 - val_acc: 0.8912\n",
            "Epoch 186/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1434 - acc: 0.9507 - val_loss: 0.3420 - val_acc: 0.8892\n",
            "Epoch 187/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.1427 - acc: 0.9507 - val_loss: 0.3472 - val_acc: 0.8888\n",
            "Epoch 188/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1426 - acc: 0.9511 - val_loss: 0.3479 - val_acc: 0.8881\n",
            "Epoch 189/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1413 - acc: 0.9518 - val_loss: 0.3419 - val_acc: 0.8888\n",
            "Epoch 190/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1406 - acc: 0.9517 - val_loss: 0.3463 - val_acc: 0.8888\n",
            "Epoch 191/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1406 - acc: 0.9515 - val_loss: 0.3611 - val_acc: 0.8842\n",
            "Epoch 192/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1399 - acc: 0.9508 - val_loss: 0.3541 - val_acc: 0.8881\n",
            "Epoch 193/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1389 - acc: 0.9523 - val_loss: 0.3454 - val_acc: 0.8890\n",
            "Epoch 194/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1393 - acc: 0.9516 - val_loss: 0.3509 - val_acc: 0.8894\n",
            "Epoch 195/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1382 - acc: 0.9528 - val_loss: 0.3484 - val_acc: 0.8886\n",
            "Epoch 196/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1367 - acc: 0.9531 - val_loss: 0.3491 - val_acc: 0.8883\n",
            "Epoch 197/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1372 - acc: 0.9532 - val_loss: 0.3499 - val_acc: 0.8903\n",
            "Epoch 198/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1363 - acc: 0.9531 - val_loss: 0.3453 - val_acc: 0.8894\n",
            "Epoch 199/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1359 - acc: 0.9536 - val_loss: 0.3524 - val_acc: 0.8878\n",
            "Epoch 200/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1351 - acc: 0.9544 - val_loss: 0.3487 - val_acc: 0.8888\n",
            "Epoch 201/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1359 - acc: 0.9535 - val_loss: 0.3477 - val_acc: 0.8901\n",
            "Epoch 202/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1338 - acc: 0.9545 - val_loss: 0.3479 - val_acc: 0.8894\n",
            "Epoch 203/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1335 - acc: 0.9551 - val_loss: 0.3463 - val_acc: 0.8918\n",
            "Epoch 204/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1318 - acc: 0.9546 - val_loss: 0.3504 - val_acc: 0.8902\n",
            "Epoch 205/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1322 - acc: 0.9547 - val_loss: 0.3499 - val_acc: 0.8868\n",
            "Epoch 206/1000\n",
            "48000/48000 [==============================] - 5s 104us/sample - loss: 0.1319 - acc: 0.9549 - val_loss: 0.3504 - val_acc: 0.8895\n",
            "Epoch 207/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.1311 - acc: 0.9557 - val_loss: 0.3511 - val_acc: 0.8900\n",
            "Epoch 208/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1310 - acc: 0.9558 - val_loss: 0.3513 - val_acc: 0.8893\n",
            "Epoch 209/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1295 - acc: 0.9559 - val_loss: 0.3521 - val_acc: 0.8886\n",
            "Epoch 210/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1297 - acc: 0.9560 - val_loss: 0.3681 - val_acc: 0.8824\n",
            "Epoch 211/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.1288 - acc: 0.9563 - val_loss: 0.3552 - val_acc: 0.8900\n",
            "Epoch 212/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1285 - acc: 0.9566 - val_loss: 0.3541 - val_acc: 0.8882\n",
            "Epoch 213/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1278 - acc: 0.9568 - val_loss: 0.3612 - val_acc: 0.8882\n",
            "Epoch 214/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1281 - acc: 0.9568 - val_loss: 0.3551 - val_acc: 0.8907\n",
            "Epoch 215/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1274 - acc: 0.9570 - val_loss: 0.3544 - val_acc: 0.8875\n",
            "Epoch 216/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1263 - acc: 0.9577 - val_loss: 0.3672 - val_acc: 0.8870\n",
            "Epoch 217/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.1263 - acc: 0.9571 - val_loss: 0.3685 - val_acc: 0.8877\n",
            "Epoch 218/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1250 - acc: 0.9579 - val_loss: 0.3560 - val_acc: 0.8902\n",
            "Epoch 219/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1249 - acc: 0.9580 - val_loss: 0.3582 - val_acc: 0.8903\n",
            "Epoch 220/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1255 - acc: 0.9574 - val_loss: 0.3590 - val_acc: 0.8885\n",
            "Epoch 221/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1228 - acc: 0.9578 - val_loss: 0.3555 - val_acc: 0.8887\n",
            "Epoch 222/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1222 - acc: 0.9582 - val_loss: 0.3586 - val_acc: 0.8898\n",
            "Epoch 223/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1226 - acc: 0.9585 - val_loss: 0.3562 - val_acc: 0.8901\n",
            "Epoch 224/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1224 - acc: 0.9591 - val_loss: 0.3612 - val_acc: 0.8882\n",
            "Epoch 225/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1211 - acc: 0.9593 - val_loss: 0.3680 - val_acc: 0.8873\n",
            "Epoch 226/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1212 - acc: 0.9591 - val_loss: 0.3578 - val_acc: 0.8891\n",
            "Epoch 227/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1205 - acc: 0.9595 - val_loss: 0.3701 - val_acc: 0.8870\n",
            "Epoch 228/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1198 - acc: 0.9600 - val_loss: 0.3687 - val_acc: 0.8880\n",
            "Epoch 229/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1196 - acc: 0.9603 - val_loss: 0.3651 - val_acc: 0.8882\n",
            "Epoch 230/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1190 - acc: 0.9601 - val_loss: 0.3643 - val_acc: 0.8891\n",
            "Epoch 231/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1180 - acc: 0.9605 - val_loss: 0.3708 - val_acc: 0.8872\n",
            "Epoch 232/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1177 - acc: 0.9604 - val_loss: 0.3701 - val_acc: 0.8883\n",
            "Epoch 233/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.1178 - acc: 0.9607 - val_loss: 0.3822 - val_acc: 0.8857\n",
            "Epoch 234/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1165 - acc: 0.9614 - val_loss: 0.3652 - val_acc: 0.8877\n",
            "Epoch 235/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1160 - acc: 0.9608 - val_loss: 0.3718 - val_acc: 0.8889\n",
            "Epoch 236/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1163 - acc: 0.9611 - val_loss: 0.3648 - val_acc: 0.8888\n",
            "Epoch 237/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1157 - acc: 0.9609 - val_loss: 0.3698 - val_acc: 0.8885\n",
            "Epoch 238/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1147 - acc: 0.9619 - val_loss: 0.3658 - val_acc: 0.8890\n",
            "Epoch 239/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1141 - acc: 0.9622 - val_loss: 0.3748 - val_acc: 0.8886\n",
            "Epoch 240/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1135 - acc: 0.9620 - val_loss: 0.3780 - val_acc: 0.8886\n",
            "Epoch 241/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.1132 - acc: 0.9627 - val_loss: 0.3697 - val_acc: 0.8899\n",
            "Epoch 242/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1130 - acc: 0.9620 - val_loss: 0.3710 - val_acc: 0.8891\n",
            "Epoch 243/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1125 - acc: 0.9624 - val_loss: 0.3880 - val_acc: 0.8850\n",
            "Epoch 244/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1117 - acc: 0.9629 - val_loss: 0.3711 - val_acc: 0.8897\n",
            "Epoch 245/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1121 - acc: 0.9620 - val_loss: 0.3785 - val_acc: 0.8886\n",
            "Epoch 246/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1113 - acc: 0.9629 - val_loss: 0.3909 - val_acc: 0.8845\n",
            "Epoch 247/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1105 - acc: 0.9632 - val_loss: 0.3793 - val_acc: 0.8895\n",
            "Epoch 248/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1103 - acc: 0.9636 - val_loss: 0.3780 - val_acc: 0.8867\n",
            "Epoch 249/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1097 - acc: 0.9635 - val_loss: 0.3867 - val_acc: 0.8861\n",
            "Epoch 250/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1093 - acc: 0.9641 - val_loss: 0.3748 - val_acc: 0.8889\n",
            "Epoch 251/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.1085 - acc: 0.9642 - val_loss: 0.3878 - val_acc: 0.8877\n",
            "Epoch 252/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1082 - acc: 0.9641 - val_loss: 0.3762 - val_acc: 0.8894\n",
            "Epoch 253/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1075 - acc: 0.9651 - val_loss: 0.3842 - val_acc: 0.8869\n",
            "Epoch 254/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1074 - acc: 0.9642 - val_loss: 0.3868 - val_acc: 0.8888\n",
            "Epoch 255/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1072 - acc: 0.9632 - val_loss: 0.3797 - val_acc: 0.8882\n",
            "Epoch 256/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1060 - acc: 0.9649 - val_loss: 0.3806 - val_acc: 0.8893\n",
            "Epoch 257/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1056 - acc: 0.9644 - val_loss: 0.3865 - val_acc: 0.8894\n",
            "Epoch 258/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1054 - acc: 0.9655 - val_loss: 0.3862 - val_acc: 0.8858\n",
            "Epoch 259/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1055 - acc: 0.9652 - val_loss: 0.3845 - val_acc: 0.8888\n",
            "Epoch 260/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.1045 - acc: 0.9657 - val_loss: 0.3841 - val_acc: 0.8889\n",
            "Epoch 261/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1046 - acc: 0.9649 - val_loss: 0.3897 - val_acc: 0.8886\n",
            "Epoch 262/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1043 - acc: 0.9653 - val_loss: 0.3903 - val_acc: 0.8888\n",
            "Epoch 263/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.1039 - acc: 0.9658 - val_loss: 0.3845 - val_acc: 0.8891\n",
            "Epoch 264/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1033 - acc: 0.9658 - val_loss: 0.3853 - val_acc: 0.8883\n",
            "Epoch 265/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1026 - acc: 0.9660 - val_loss: 0.3871 - val_acc: 0.8889\n",
            "Epoch 266/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.1023 - acc: 0.9661 - val_loss: 0.3851 - val_acc: 0.8878\n",
            "Epoch 267/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.1010 - acc: 0.9664 - val_loss: 0.3851 - val_acc: 0.8888\n",
            "Epoch 268/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1013 - acc: 0.9666 - val_loss: 0.3939 - val_acc: 0.8871\n",
            "Epoch 269/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1006 - acc: 0.9668 - val_loss: 0.3953 - val_acc: 0.8878\n",
            "Epoch 270/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.1003 - acc: 0.9668 - val_loss: 0.3939 - val_acc: 0.8892\n",
            "Epoch 271/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0995 - acc: 0.9670 - val_loss: 0.3907 - val_acc: 0.8888\n",
            "Epoch 272/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0995 - acc: 0.9674 - val_loss: 0.3971 - val_acc: 0.8869\n",
            "Epoch 273/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.0988 - acc: 0.9676 - val_loss: 0.3969 - val_acc: 0.8878\n",
            "Epoch 274/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0987 - acc: 0.9675 - val_loss: 0.3902 - val_acc: 0.8887\n",
            "Epoch 275/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0980 - acc: 0.9677 - val_loss: 0.3986 - val_acc: 0.8884\n",
            "Epoch 276/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0986 - acc: 0.9671 - val_loss: 0.3973 - val_acc: 0.8882\n",
            "Epoch 277/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0971 - acc: 0.9684 - val_loss: 0.3932 - val_acc: 0.8881\n",
            "Epoch 278/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0964 - acc: 0.9686 - val_loss: 0.3948 - val_acc: 0.8907\n",
            "Epoch 279/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0972 - acc: 0.9683 - val_loss: 0.4037 - val_acc: 0.8897\n",
            "Epoch 280/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0961 - acc: 0.9690 - val_loss: 0.4037 - val_acc: 0.8872\n",
            "Epoch 281/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0954 - acc: 0.9685 - val_loss: 0.4124 - val_acc: 0.8839\n",
            "Epoch 282/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0953 - acc: 0.9687 - val_loss: 0.4011 - val_acc: 0.8871\n",
            "Epoch 283/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0946 - acc: 0.9691 - val_loss: 0.4107 - val_acc: 0.8871\n",
            "Epoch 284/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0942 - acc: 0.9694 - val_loss: 0.4198 - val_acc: 0.8817\n",
            "Epoch 285/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0937 - acc: 0.9694 - val_loss: 0.4086 - val_acc: 0.8882\n",
            "Epoch 286/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0949 - acc: 0.9683 - val_loss: 0.4058 - val_acc: 0.8867\n",
            "Epoch 287/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0926 - acc: 0.9700 - val_loss: 0.3974 - val_acc: 0.8892\n",
            "Epoch 288/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0931 - acc: 0.9694 - val_loss: 0.4037 - val_acc: 0.8886\n",
            "Epoch 289/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0920 - acc: 0.9701 - val_loss: 0.4018 - val_acc: 0.8889\n",
            "Epoch 290/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0922 - acc: 0.9704 - val_loss: 0.4062 - val_acc: 0.8890\n",
            "Epoch 291/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0911 - acc: 0.9708 - val_loss: 0.4021 - val_acc: 0.8885\n",
            "Epoch 292/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0914 - acc: 0.9704 - val_loss: 0.4149 - val_acc: 0.8875\n",
            "Epoch 293/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0912 - acc: 0.9701 - val_loss: 0.4093 - val_acc: 0.8873\n",
            "Epoch 294/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0907 - acc: 0.9706 - val_loss: 0.4122 - val_acc: 0.8878\n",
            "Epoch 295/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0907 - acc: 0.9703 - val_loss: 0.4070 - val_acc: 0.8882\n",
            "Epoch 296/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0891 - acc: 0.9716 - val_loss: 0.4108 - val_acc: 0.8879\n",
            "Epoch 297/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0892 - acc: 0.9715 - val_loss: 0.4131 - val_acc: 0.8874\n",
            "Epoch 298/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0892 - acc: 0.9719 - val_loss: 0.4103 - val_acc: 0.8863\n",
            "Epoch 299/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0887 - acc: 0.9715 - val_loss: 0.4205 - val_acc: 0.8867\n",
            "Epoch 300/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0887 - acc: 0.9711 - val_loss: 0.4194 - val_acc: 0.8882\n",
            "Epoch 301/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0876 - acc: 0.9721 - val_loss: 0.4155 - val_acc: 0.8891\n",
            "Epoch 302/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0874 - acc: 0.9721 - val_loss: 0.4115 - val_acc: 0.8883\n",
            "Epoch 303/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0875 - acc: 0.9715 - val_loss: 0.4150 - val_acc: 0.8864\n",
            "Epoch 304/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0870 - acc: 0.9713 - val_loss: 0.4419 - val_acc: 0.8825\n",
            "Epoch 305/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0858 - acc: 0.9728 - val_loss: 0.4135 - val_acc: 0.8895\n",
            "Epoch 306/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0853 - acc: 0.9727 - val_loss: 0.4191 - val_acc: 0.8887\n",
            "Epoch 307/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0850 - acc: 0.9736 - val_loss: 0.4193 - val_acc: 0.8880\n",
            "Epoch 308/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0860 - acc: 0.9727 - val_loss: 0.4156 - val_acc: 0.8892\n",
            "Epoch 309/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0858 - acc: 0.9724 - val_loss: 0.4171 - val_acc: 0.8892\n",
            "Epoch 310/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0841 - acc: 0.9735 - val_loss: 0.4154 - val_acc: 0.8895\n",
            "Epoch 311/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0841 - acc: 0.9730 - val_loss: 0.4234 - val_acc: 0.8868\n",
            "Epoch 312/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0835 - acc: 0.9733 - val_loss: 0.4186 - val_acc: 0.8885\n",
            "Epoch 313/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0833 - acc: 0.9735 - val_loss: 0.4186 - val_acc: 0.8891\n",
            "Epoch 314/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0823 - acc: 0.9736 - val_loss: 0.4311 - val_acc: 0.8867\n",
            "Epoch 315/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0830 - acc: 0.9731 - val_loss: 0.4215 - val_acc: 0.8904\n",
            "Epoch 316/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0828 - acc: 0.9735 - val_loss: 0.4231 - val_acc: 0.8892\n",
            "Epoch 317/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0815 - acc: 0.9741 - val_loss: 0.4334 - val_acc: 0.8871\n",
            "Epoch 318/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0814 - acc: 0.9740 - val_loss: 0.4296 - val_acc: 0.8882\n",
            "Epoch 319/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0812 - acc: 0.9739 - val_loss: 0.4279 - val_acc: 0.8880\n",
            "Epoch 320/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0817 - acc: 0.9737 - val_loss: 0.4313 - val_acc: 0.8867\n",
            "Epoch 321/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0802 - acc: 0.9748 - val_loss: 0.4305 - val_acc: 0.8872\n",
            "Epoch 322/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0798 - acc: 0.9745 - val_loss: 0.4304 - val_acc: 0.8878\n",
            "Epoch 323/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0797 - acc: 0.9753 - val_loss: 0.4401 - val_acc: 0.8858\n",
            "Epoch 324/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0801 - acc: 0.9744 - val_loss: 0.4289 - val_acc: 0.8880\n",
            "Epoch 325/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0788 - acc: 0.9752 - val_loss: 0.4292 - val_acc: 0.8887\n",
            "Epoch 326/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0790 - acc: 0.9744 - val_loss: 0.4464 - val_acc: 0.8853\n",
            "Epoch 327/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0787 - acc: 0.9748 - val_loss: 0.4374 - val_acc: 0.8874\n",
            "Epoch 328/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0780 - acc: 0.9761 - val_loss: 0.4439 - val_acc: 0.8867\n",
            "Epoch 329/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0779 - acc: 0.9753 - val_loss: 0.4371 - val_acc: 0.8873\n",
            "Epoch 330/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0773 - acc: 0.9756 - val_loss: 0.4351 - val_acc: 0.8875\n",
            "Epoch 331/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0771 - acc: 0.9756 - val_loss: 0.4319 - val_acc: 0.8903\n",
            "Epoch 332/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0773 - acc: 0.9759 - val_loss: 0.4303 - val_acc: 0.8890\n",
            "Epoch 333/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0762 - acc: 0.9763 - val_loss: 0.4334 - val_acc: 0.8882\n",
            "Epoch 334/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0763 - acc: 0.9757 - val_loss: 0.4377 - val_acc: 0.8868\n",
            "Epoch 335/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0754 - acc: 0.9765 - val_loss: 0.4402 - val_acc: 0.8877\n",
            "Epoch 336/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0752 - acc: 0.9770 - val_loss: 0.4345 - val_acc: 0.8897\n",
            "Epoch 337/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0747 - acc: 0.9769 - val_loss: 0.4394 - val_acc: 0.8886\n",
            "Epoch 338/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0754 - acc: 0.9768 - val_loss: 0.4501 - val_acc: 0.8857\n",
            "Epoch 339/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0748 - acc: 0.9771 - val_loss: 0.4484 - val_acc: 0.8867\n",
            "Epoch 340/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0743 - acc: 0.9767 - val_loss: 0.4485 - val_acc: 0.8863\n",
            "Epoch 341/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.0731 - acc: 0.9776 - val_loss: 0.4425 - val_acc: 0.8898\n",
            "Epoch 342/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0732 - acc: 0.9771 - val_loss: 0.4445 - val_acc: 0.8877\n",
            "Epoch 343/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0734 - acc: 0.9771 - val_loss: 0.4649 - val_acc: 0.8811\n",
            "Epoch 344/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0731 - acc: 0.9769 - val_loss: 0.4393 - val_acc: 0.8894\n",
            "Epoch 345/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0730 - acc: 0.9779 - val_loss: 0.4531 - val_acc: 0.8869\n",
            "Epoch 346/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0718 - acc: 0.9779 - val_loss: 0.4485 - val_acc: 0.8879\n",
            "Epoch 347/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0719 - acc: 0.9775 - val_loss: 0.4614 - val_acc: 0.8845\n",
            "Epoch 348/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0716 - acc: 0.9779 - val_loss: 0.4535 - val_acc: 0.8874\n",
            "Epoch 349/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0714 - acc: 0.9778 - val_loss: 0.4617 - val_acc: 0.8862\n",
            "Epoch 350/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0705 - acc: 0.9784 - val_loss: 0.4664 - val_acc: 0.8880\n",
            "Epoch 351/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0714 - acc: 0.9770 - val_loss: 0.4545 - val_acc: 0.8868\n",
            "Epoch 352/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0700 - acc: 0.9787 - val_loss: 0.4539 - val_acc: 0.8853\n",
            "Epoch 353/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0705 - acc: 0.9777 - val_loss: 0.4497 - val_acc: 0.8872\n",
            "Epoch 354/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0696 - acc: 0.9785 - val_loss: 0.4504 - val_acc: 0.8875\n",
            "Epoch 355/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0698 - acc: 0.9792 - val_loss: 0.4518 - val_acc: 0.8878\n",
            "Epoch 356/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0696 - acc: 0.9786 - val_loss: 0.4519 - val_acc: 0.8879\n",
            "Epoch 357/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0684 - acc: 0.9781 - val_loss: 0.4665 - val_acc: 0.8859\n",
            "Epoch 358/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0685 - acc: 0.9788 - val_loss: 0.4607 - val_acc: 0.8872\n",
            "Epoch 359/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0682 - acc: 0.9791 - val_loss: 0.4592 - val_acc: 0.8878\n",
            "Epoch 360/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0681 - acc: 0.9785 - val_loss: 0.4607 - val_acc: 0.8875\n",
            "Epoch 361/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0679 - acc: 0.9787 - val_loss: 0.4584 - val_acc: 0.8870\n",
            "Epoch 362/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0668 - acc: 0.9796 - val_loss: 0.4575 - val_acc: 0.8887\n",
            "Epoch 363/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0669 - acc: 0.9798 - val_loss: 0.4654 - val_acc: 0.8854\n",
            "Epoch 364/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0662 - acc: 0.9804 - val_loss: 0.4624 - val_acc: 0.8883\n",
            "Epoch 365/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0672 - acc: 0.9785 - val_loss: 0.4640 - val_acc: 0.8863\n",
            "Epoch 366/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0657 - acc: 0.9794 - val_loss: 0.4644 - val_acc: 0.8865\n",
            "Epoch 367/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0649 - acc: 0.9809 - val_loss: 0.4642 - val_acc: 0.8876\n",
            "Epoch 368/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0658 - acc: 0.9801 - val_loss: 0.4623 - val_acc: 0.8899\n",
            "Epoch 369/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0650 - acc: 0.9807 - val_loss: 0.4664 - val_acc: 0.8876\n",
            "Epoch 370/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0645 - acc: 0.9797 - val_loss: 0.4744 - val_acc: 0.8870\n",
            "Epoch 371/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0642 - acc: 0.9806 - val_loss: 0.4691 - val_acc: 0.8888\n",
            "Epoch 372/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0644 - acc: 0.9802 - val_loss: 0.4723 - val_acc: 0.8873\n",
            "Epoch 373/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0640 - acc: 0.9808 - val_loss: 0.4772 - val_acc: 0.8859\n",
            "Epoch 374/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0637 - acc: 0.9810 - val_loss: 0.4822 - val_acc: 0.8851\n",
            "Epoch 375/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0637 - acc: 0.9803 - val_loss: 0.4731 - val_acc: 0.8874\n",
            "Epoch 376/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0635 - acc: 0.9812 - val_loss: 0.4729 - val_acc: 0.8866\n",
            "Epoch 377/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0628 - acc: 0.9814 - val_loss: 0.4804 - val_acc: 0.8851\n",
            "Epoch 378/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0623 - acc: 0.9809 - val_loss: 0.4855 - val_acc: 0.8846\n",
            "Epoch 379/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0621 - acc: 0.9809 - val_loss: 0.4792 - val_acc: 0.8876\n",
            "Epoch 380/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0618 - acc: 0.9814 - val_loss: 0.4759 - val_acc: 0.8882\n",
            "Epoch 381/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0622 - acc: 0.9806 - val_loss: 0.4902 - val_acc: 0.8878\n",
            "Epoch 382/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0611 - acc: 0.9820 - val_loss: 0.4775 - val_acc: 0.8864\n",
            "Epoch 383/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0611 - acc: 0.9819 - val_loss: 0.4829 - val_acc: 0.8874\n",
            "Epoch 384/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0605 - acc: 0.9824 - val_loss: 0.4929 - val_acc: 0.8832\n",
            "Epoch 385/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0605 - acc: 0.9823 - val_loss: 0.4812 - val_acc: 0.8861\n",
            "Epoch 386/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0605 - acc: 0.9816 - val_loss: 0.4980 - val_acc: 0.8843\n",
            "Epoch 387/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0603 - acc: 0.9820 - val_loss: 0.4833 - val_acc: 0.8871\n",
            "Epoch 388/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0592 - acc: 0.9828 - val_loss: 0.4877 - val_acc: 0.8864\n",
            "Epoch 389/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0588 - acc: 0.9826 - val_loss: 0.4881 - val_acc: 0.8852\n",
            "Epoch 390/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0591 - acc: 0.9825 - val_loss: 0.4819 - val_acc: 0.8875\n",
            "Epoch 391/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0592 - acc: 0.9824 - val_loss: 0.4964 - val_acc: 0.8852\n",
            "Epoch 392/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0587 - acc: 0.9823 - val_loss: 0.4889 - val_acc: 0.8863\n",
            "Epoch 393/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0592 - acc: 0.9829 - val_loss: 0.4854 - val_acc: 0.8869\n",
            "Epoch 394/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0574 - acc: 0.9838 - val_loss: 0.4895 - val_acc: 0.8863\n",
            "Epoch 395/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0572 - acc: 0.9834 - val_loss: 0.4853 - val_acc: 0.8882\n",
            "Epoch 396/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0575 - acc: 0.9833 - val_loss: 0.4974 - val_acc: 0.8877\n",
            "Epoch 397/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0575 - acc: 0.9826 - val_loss: 0.4955 - val_acc: 0.8867\n",
            "Epoch 398/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0568 - acc: 0.9837 - val_loss: 0.4934 - val_acc: 0.8875\n",
            "Epoch 399/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0571 - acc: 0.9834 - val_loss: 0.5054 - val_acc: 0.8862\n",
            "Epoch 400/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0558 - acc: 0.9836 - val_loss: 0.4947 - val_acc: 0.8863\n",
            "Epoch 401/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0567 - acc: 0.9831 - val_loss: 0.4964 - val_acc: 0.8871\n",
            "Epoch 402/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0562 - acc: 0.9831 - val_loss: 0.4996 - val_acc: 0.8862\n",
            "Epoch 403/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0558 - acc: 0.9839 - val_loss: 0.4929 - val_acc: 0.8892\n",
            "Epoch 404/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0550 - acc: 0.9843 - val_loss: 0.4939 - val_acc: 0.8891\n",
            "Epoch 405/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0545 - acc: 0.9844 - val_loss: 0.5052 - val_acc: 0.8860\n",
            "Epoch 406/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0548 - acc: 0.9841 - val_loss: 0.5001 - val_acc: 0.8868\n",
            "Epoch 407/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0552 - acc: 0.9835 - val_loss: 0.4955 - val_acc: 0.8880\n",
            "Epoch 408/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0542 - acc: 0.9847 - val_loss: 0.4965 - val_acc: 0.8882\n",
            "Epoch 409/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0539 - acc: 0.9846 - val_loss: 0.5114 - val_acc: 0.8848\n",
            "Epoch 410/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0533 - acc: 0.9846 - val_loss: 0.4975 - val_acc: 0.8863\n",
            "Epoch 411/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0538 - acc: 0.9847 - val_loss: 0.5014 - val_acc: 0.8878\n",
            "Epoch 412/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0529 - acc: 0.9846 - val_loss: 0.5035 - val_acc: 0.8888\n",
            "Epoch 413/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0527 - acc: 0.9847 - val_loss: 0.5007 - val_acc: 0.8888\n",
            "Epoch 414/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0524 - acc: 0.9852 - val_loss: 0.5064 - val_acc: 0.8868\n",
            "Epoch 415/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0523 - acc: 0.9848 - val_loss: 0.5008 - val_acc: 0.8873\n",
            "Epoch 416/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0526 - acc: 0.9848 - val_loss: 0.5084 - val_acc: 0.8850\n",
            "Epoch 417/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0522 - acc: 0.9850 - val_loss: 0.5050 - val_acc: 0.8868\n",
            "Epoch 418/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0511 - acc: 0.9853 - val_loss: 0.5077 - val_acc: 0.8871\n",
            "Epoch 419/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0514 - acc: 0.9851 - val_loss: 0.5071 - val_acc: 0.8875\n",
            "Epoch 420/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0512 - acc: 0.9854 - val_loss: 0.5165 - val_acc: 0.8867\n",
            "Epoch 421/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0510 - acc: 0.9849 - val_loss: 0.5096 - val_acc: 0.8869\n",
            "Epoch 422/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0508 - acc: 0.9858 - val_loss: 0.5087 - val_acc: 0.8878\n",
            "Epoch 423/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0502 - acc: 0.9860 - val_loss: 0.5127 - val_acc: 0.8864\n",
            "Epoch 424/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0498 - acc: 0.9858 - val_loss: 0.5153 - val_acc: 0.8889\n",
            "Epoch 425/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0499 - acc: 0.9862 - val_loss: 0.5145 - val_acc: 0.8871\n",
            "Epoch 426/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0504 - acc: 0.9854 - val_loss: 0.5316 - val_acc: 0.8838\n",
            "Epoch 427/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0494 - acc: 0.9862 - val_loss: 0.5208 - val_acc: 0.8869\n",
            "Epoch 428/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0496 - acc: 0.9856 - val_loss: 0.5114 - val_acc: 0.8882\n",
            "Epoch 429/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0486 - acc: 0.9866 - val_loss: 0.5174 - val_acc: 0.8873\n",
            "Epoch 430/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0495 - acc: 0.9858 - val_loss: 0.5157 - val_acc: 0.8883\n",
            "Epoch 431/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0486 - acc: 0.9862 - val_loss: 0.5166 - val_acc: 0.8890\n",
            "Epoch 432/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0483 - acc: 0.9862 - val_loss: 0.5183 - val_acc: 0.8878\n",
            "Epoch 433/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0484 - acc: 0.9861 - val_loss: 0.5209 - val_acc: 0.8874\n",
            "Epoch 434/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0482 - acc: 0.9860 - val_loss: 0.5272 - val_acc: 0.8857\n",
            "Epoch 435/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0480 - acc: 0.9871 - val_loss: 0.5246 - val_acc: 0.8875\n",
            "Epoch 436/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0472 - acc: 0.9865 - val_loss: 0.5289 - val_acc: 0.8858\n",
            "Epoch 437/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0475 - acc: 0.9866 - val_loss: 0.5212 - val_acc: 0.8873\n",
            "Epoch 438/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0467 - acc: 0.9874 - val_loss: 0.5252 - val_acc: 0.8877\n",
            "Epoch 439/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0461 - acc: 0.9875 - val_loss: 0.5279 - val_acc: 0.8878\n",
            "Epoch 440/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0476 - acc: 0.9863 - val_loss: 0.5336 - val_acc: 0.8859\n",
            "Epoch 441/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0466 - acc: 0.9866 - val_loss: 0.5294 - val_acc: 0.8857\n",
            "Epoch 442/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0465 - acc: 0.9874 - val_loss: 0.5268 - val_acc: 0.8870\n",
            "Epoch 443/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0451 - acc: 0.9874 - val_loss: 0.5425 - val_acc: 0.8847\n",
            "Epoch 444/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0455 - acc: 0.9877 - val_loss: 0.5388 - val_acc: 0.8839\n",
            "Epoch 445/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0450 - acc: 0.9874 - val_loss: 0.5365 - val_acc: 0.8866\n",
            "Epoch 446/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0455 - acc: 0.9877 - val_loss: 0.5330 - val_acc: 0.8872\n",
            "Epoch 447/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0452 - acc: 0.9873 - val_loss: 0.5400 - val_acc: 0.8877\n",
            "Epoch 448/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0446 - acc: 0.9879 - val_loss: 0.5367 - val_acc: 0.8856\n",
            "Epoch 449/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0439 - acc: 0.9882 - val_loss: 0.5459 - val_acc: 0.8852\n",
            "Epoch 450/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0445 - acc: 0.9878 - val_loss: 0.5327 - val_acc: 0.8872\n",
            "Epoch 451/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0453 - acc: 0.9873 - val_loss: 0.5419 - val_acc: 0.8886\n",
            "Epoch 452/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0433 - acc: 0.9885 - val_loss: 0.5340 - val_acc: 0.8872\n",
            "Epoch 453/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0435 - acc: 0.9882 - val_loss: 0.5504 - val_acc: 0.8840\n",
            "Epoch 454/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0432 - acc: 0.9885 - val_loss: 0.5512 - val_acc: 0.8860\n",
            "Epoch 455/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0433 - acc: 0.9883 - val_loss: 0.5363 - val_acc: 0.8882\n",
            "Epoch 456/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0430 - acc: 0.9883 - val_loss: 0.5399 - val_acc: 0.8876\n",
            "Epoch 457/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0428 - acc: 0.9881 - val_loss: 0.5502 - val_acc: 0.8862\n",
            "Epoch 458/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0426 - acc: 0.9887 - val_loss: 0.5464 - val_acc: 0.8846\n",
            "Epoch 459/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0434 - acc: 0.9876 - val_loss: 0.5474 - val_acc: 0.8851\n",
            "Epoch 460/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0414 - acc: 0.9889 - val_loss: 0.5483 - val_acc: 0.8863\n",
            "Epoch 461/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0416 - acc: 0.9884 - val_loss: 0.5504 - val_acc: 0.8868\n",
            "Epoch 462/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0423 - acc: 0.9885 - val_loss: 0.5515 - val_acc: 0.8861\n",
            "Epoch 463/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0416 - acc: 0.9891 - val_loss: 0.5504 - val_acc: 0.8875\n",
            "Epoch 464/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0409 - acc: 0.9889 - val_loss: 0.5524 - val_acc: 0.8850\n",
            "Epoch 465/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0411 - acc: 0.9889 - val_loss: 0.5648 - val_acc: 0.8839\n",
            "Epoch 466/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0408 - acc: 0.9889 - val_loss: 0.5534 - val_acc: 0.8897\n",
            "Epoch 467/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0401 - acc: 0.9894 - val_loss: 0.5500 - val_acc: 0.8876\n",
            "Epoch 468/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0406 - acc: 0.9891 - val_loss: 0.5605 - val_acc: 0.8862\n",
            "Epoch 469/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0403 - acc: 0.9892 - val_loss: 0.5559 - val_acc: 0.8873\n",
            "Epoch 470/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0398 - acc: 0.9891 - val_loss: 0.5595 - val_acc: 0.8870\n",
            "Epoch 471/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0391 - acc: 0.9899 - val_loss: 0.5742 - val_acc: 0.8830\n",
            "Epoch 472/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0395 - acc: 0.9893 - val_loss: 0.5671 - val_acc: 0.8861\n",
            "Epoch 473/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0397 - acc: 0.9897 - val_loss: 0.5611 - val_acc: 0.8860\n",
            "Epoch 474/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0390 - acc: 0.9898 - val_loss: 0.5584 - val_acc: 0.8866\n",
            "Epoch 475/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0388 - acc: 0.9897 - val_loss: 0.5613 - val_acc: 0.8875\n",
            "Epoch 476/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0395 - acc: 0.9897 - val_loss: 0.5661 - val_acc: 0.8846\n",
            "Epoch 477/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0379 - acc: 0.9902 - val_loss: 0.5565 - val_acc: 0.8875\n",
            "Epoch 478/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.0383 - acc: 0.9900 - val_loss: 0.5689 - val_acc: 0.8849\n",
            "Epoch 479/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0383 - acc: 0.9899 - val_loss: 0.5616 - val_acc: 0.8856\n",
            "Epoch 480/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0384 - acc: 0.9901 - val_loss: 0.5689 - val_acc: 0.8858\n",
            "Epoch 481/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0374 - acc: 0.9903 - val_loss: 0.5662 - val_acc: 0.8881\n",
            "Epoch 482/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0379 - acc: 0.9902 - val_loss: 0.5832 - val_acc: 0.8827\n",
            "Epoch 483/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0375 - acc: 0.9906 - val_loss: 0.5682 - val_acc: 0.8867\n",
            "Epoch 484/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0375 - acc: 0.9901 - val_loss: 0.5667 - val_acc: 0.8865\n",
            "Epoch 485/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0371 - acc: 0.9904 - val_loss: 0.5679 - val_acc: 0.8873\n",
            "Epoch 486/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0369 - acc: 0.9902 - val_loss: 0.5734 - val_acc: 0.8866\n",
            "Epoch 487/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0368 - acc: 0.9903 - val_loss: 0.5711 - val_acc: 0.8863\n",
            "Epoch 488/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0355 - acc: 0.9915 - val_loss: 0.5709 - val_acc: 0.8882\n",
            "Epoch 489/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0367 - acc: 0.9904 - val_loss: 0.5729 - val_acc: 0.8868\n",
            "Epoch 490/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0354 - acc: 0.9913 - val_loss: 0.5827 - val_acc: 0.8860\n",
            "Epoch 491/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0351 - acc: 0.9918 - val_loss: 0.5786 - val_acc: 0.8875\n",
            "Epoch 492/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0363 - acc: 0.9908 - val_loss: 0.5759 - val_acc: 0.8875\n",
            "Epoch 493/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0350 - acc: 0.9918 - val_loss: 0.5769 - val_acc: 0.8866\n",
            "Epoch 494/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0358 - acc: 0.9908 - val_loss: 0.5775 - val_acc: 0.8865\n",
            "Epoch 495/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0348 - acc: 0.9912 - val_loss: 0.5745 - val_acc: 0.8877\n",
            "Epoch 496/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0353 - acc: 0.9909 - val_loss: 0.5821 - val_acc: 0.8845\n",
            "Epoch 497/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0350 - acc: 0.9916 - val_loss: 0.5798 - val_acc: 0.8869\n",
            "Epoch 498/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0347 - acc: 0.9915 - val_loss: 0.5912 - val_acc: 0.8858\n",
            "Epoch 499/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0339 - acc: 0.9912 - val_loss: 0.5796 - val_acc: 0.8863\n",
            "Epoch 500/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0347 - acc: 0.9909 - val_loss: 0.6120 - val_acc: 0.8821\n",
            "Epoch 501/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0347 - acc: 0.9912 - val_loss: 0.5853 - val_acc: 0.8845\n",
            "Epoch 502/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0334 - acc: 0.9921 - val_loss: 0.5877 - val_acc: 0.8866\n",
            "Epoch 503/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0339 - acc: 0.9918 - val_loss: 0.5978 - val_acc: 0.8833\n",
            "Epoch 504/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0330 - acc: 0.9916 - val_loss: 0.5880 - val_acc: 0.8871\n",
            "Epoch 505/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0331 - acc: 0.9919 - val_loss: 0.5925 - val_acc: 0.8852\n",
            "Epoch 506/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0334 - acc: 0.9917 - val_loss: 0.6116 - val_acc: 0.8801\n",
            "Epoch 507/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0321 - acc: 0.9923 - val_loss: 0.5879 - val_acc: 0.8880\n",
            "Epoch 508/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0329 - acc: 0.9918 - val_loss: 0.5917 - val_acc: 0.8850\n",
            "Epoch 509/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0332 - acc: 0.9918 - val_loss: 0.5971 - val_acc: 0.8842\n",
            "Epoch 510/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0324 - acc: 0.9924 - val_loss: 0.5966 - val_acc: 0.8863\n",
            "Epoch 511/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0320 - acc: 0.9921 - val_loss: 0.6083 - val_acc: 0.8848\n",
            "Epoch 512/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0324 - acc: 0.9920 - val_loss: 0.5937 - val_acc: 0.8861\n",
            "Epoch 513/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0322 - acc: 0.9918 - val_loss: 0.5974 - val_acc: 0.8851\n",
            "Epoch 514/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0315 - acc: 0.9923 - val_loss: 0.6029 - val_acc: 0.8855\n",
            "Epoch 515/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0323 - acc: 0.9922 - val_loss: 0.6001 - val_acc: 0.8889\n",
            "Epoch 516/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0311 - acc: 0.9926 - val_loss: 0.6013 - val_acc: 0.8871\n",
            "Epoch 517/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0326 - acc: 0.9919 - val_loss: 0.5991 - val_acc: 0.8866\n",
            "Epoch 518/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0300 - acc: 0.9927 - val_loss: 0.6100 - val_acc: 0.8842\n",
            "Epoch 519/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0301 - acc: 0.9931 - val_loss: 0.6011 - val_acc: 0.8875\n",
            "Epoch 520/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0311 - acc: 0.9927 - val_loss: 0.6103 - val_acc: 0.8862\n",
            "Epoch 521/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0306 - acc: 0.9927 - val_loss: 0.6046 - val_acc: 0.8865\n",
            "Epoch 522/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0300 - acc: 0.9933 - val_loss: 0.6076 - val_acc: 0.8874\n",
            "Epoch 523/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0303 - acc: 0.9930 - val_loss: 0.6038 - val_acc: 0.8870\n",
            "Epoch 524/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0295 - acc: 0.9932 - val_loss: 0.6220 - val_acc: 0.8838\n",
            "Epoch 525/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0300 - acc: 0.9929 - val_loss: 0.6098 - val_acc: 0.8867\n",
            "Epoch 526/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0287 - acc: 0.9935 - val_loss: 0.6238 - val_acc: 0.8829\n",
            "Epoch 527/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0292 - acc: 0.9937 - val_loss: 0.6148 - val_acc: 0.8844\n",
            "Epoch 528/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0302 - acc: 0.9927 - val_loss: 0.6179 - val_acc: 0.8851\n",
            "Epoch 529/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0283 - acc: 0.9938 - val_loss: 0.6162 - val_acc: 0.8864\n",
            "Epoch 530/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0291 - acc: 0.9930 - val_loss: 0.6081 - val_acc: 0.8873\n",
            "Epoch 531/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0299 - acc: 0.9929 - val_loss: 0.6115 - val_acc: 0.8851\n",
            "Epoch 532/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0285 - acc: 0.9935 - val_loss: 0.6116 - val_acc: 0.8871\n",
            "Epoch 533/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0291 - acc: 0.9932 - val_loss: 0.6197 - val_acc: 0.8860\n",
            "Epoch 534/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0282 - acc: 0.9933 - val_loss: 0.6162 - val_acc: 0.8882\n",
            "Epoch 535/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0283 - acc: 0.9936 - val_loss: 0.6169 - val_acc: 0.8858\n",
            "Epoch 536/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0281 - acc: 0.9933 - val_loss: 0.6227 - val_acc: 0.8840\n",
            "Epoch 537/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0278 - acc: 0.9935 - val_loss: 0.6207 - val_acc: 0.8868\n",
            "Epoch 538/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0278 - acc: 0.9934 - val_loss: 0.6404 - val_acc: 0.8821\n",
            "Epoch 539/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0276 - acc: 0.9937 - val_loss: 0.6214 - val_acc: 0.8872\n",
            "Epoch 540/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0272 - acc: 0.9940 - val_loss: 0.6262 - val_acc: 0.8848\n",
            "Epoch 541/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0273 - acc: 0.9939 - val_loss: 0.6245 - val_acc: 0.8867\n",
            "Epoch 542/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0274 - acc: 0.9935 - val_loss: 0.6275 - val_acc: 0.8870\n",
            "Epoch 543/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0270 - acc: 0.9939 - val_loss: 0.6322 - val_acc: 0.8834\n",
            "Epoch 544/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0270 - acc: 0.9939 - val_loss: 0.6301 - val_acc: 0.8868\n",
            "Epoch 545/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0269 - acc: 0.9938 - val_loss: 0.6383 - val_acc: 0.8850\n",
            "Epoch 546/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0267 - acc: 0.9937 - val_loss: 0.6385 - val_acc: 0.8862\n",
            "Epoch 547/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0263 - acc: 0.9941 - val_loss: 0.6301 - val_acc: 0.8876\n",
            "Epoch 548/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0260 - acc: 0.9941 - val_loss: 0.6314 - val_acc: 0.8882\n",
            "Epoch 549/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0262 - acc: 0.9941 - val_loss: 0.6356 - val_acc: 0.8857\n",
            "Epoch 550/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0269 - acc: 0.9934 - val_loss: 0.6409 - val_acc: 0.8848\n",
            "Epoch 551/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0259 - acc: 0.9944 - val_loss: 0.6487 - val_acc: 0.8852\n",
            "Epoch 552/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0258 - acc: 0.9942 - val_loss: 0.6388 - val_acc: 0.8849\n",
            "Epoch 553/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0261 - acc: 0.9940 - val_loss: 0.6389 - val_acc: 0.8857\n",
            "Epoch 554/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0247 - acc: 0.9945 - val_loss: 0.6477 - val_acc: 0.8820\n",
            "Epoch 555/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0256 - acc: 0.9942 - val_loss: 0.6414 - val_acc: 0.8848\n",
            "Epoch 556/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0249 - acc: 0.9947 - val_loss: 0.6385 - val_acc: 0.8863\n",
            "Epoch 557/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0250 - acc: 0.9950 - val_loss: 0.6522 - val_acc: 0.8830\n",
            "Epoch 558/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0250 - acc: 0.9944 - val_loss: 0.6422 - val_acc: 0.8873\n",
            "Epoch 559/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0243 - acc: 0.9950 - val_loss: 0.6437 - val_acc: 0.8858\n",
            "Epoch 560/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0243 - acc: 0.9948 - val_loss: 0.6456 - val_acc: 0.8853\n",
            "Epoch 561/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0242 - acc: 0.9950 - val_loss: 0.6471 - val_acc: 0.8851\n",
            "Epoch 562/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0242 - acc: 0.9947 - val_loss: 0.6420 - val_acc: 0.8867\n",
            "Epoch 563/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0241 - acc: 0.9949 - val_loss: 0.6455 - val_acc: 0.8843\n",
            "Epoch 564/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0241 - acc: 0.9952 - val_loss: 0.6470 - val_acc: 0.8865\n",
            "Epoch 565/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0240 - acc: 0.9949 - val_loss: 0.6537 - val_acc: 0.8846\n",
            "Epoch 566/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0237 - acc: 0.9949 - val_loss: 0.6603 - val_acc: 0.8829\n",
            "Epoch 567/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0243 - acc: 0.9947 - val_loss: 0.6589 - val_acc: 0.8847\n",
            "Epoch 568/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0232 - acc: 0.9952 - val_loss: 0.6679 - val_acc: 0.8825\n",
            "Epoch 569/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0226 - acc: 0.9958 - val_loss: 0.6732 - val_acc: 0.8810\n",
            "Epoch 570/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0232 - acc: 0.9952 - val_loss: 0.6573 - val_acc: 0.8868\n",
            "Epoch 571/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0229 - acc: 0.9953 - val_loss: 0.6547 - val_acc: 0.8856\n",
            "Epoch 572/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0227 - acc: 0.9953 - val_loss: 0.6592 - val_acc: 0.8852\n",
            "Epoch 573/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0235 - acc: 0.9949 - val_loss: 0.6557 - val_acc: 0.8858\n",
            "Epoch 574/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0222 - acc: 0.9956 - val_loss: 0.6759 - val_acc: 0.8817\n",
            "Epoch 575/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0227 - acc: 0.9951 - val_loss: 0.6599 - val_acc: 0.8829\n",
            "Epoch 576/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0223 - acc: 0.9953 - val_loss: 0.6616 - val_acc: 0.8857\n",
            "Epoch 577/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0223 - acc: 0.9955 - val_loss: 0.6675 - val_acc: 0.8833\n",
            "Epoch 578/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0218 - acc: 0.9956 - val_loss: 0.6689 - val_acc: 0.8852\n",
            "Epoch 579/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0223 - acc: 0.9956 - val_loss: 0.6619 - val_acc: 0.8858\n",
            "Epoch 580/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0220 - acc: 0.9956 - val_loss: 0.6709 - val_acc: 0.8848\n",
            "Epoch 581/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0221 - acc: 0.9955 - val_loss: 0.6608 - val_acc: 0.8840\n",
            "Epoch 582/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0218 - acc: 0.9956 - val_loss: 0.6641 - val_acc: 0.8863\n",
            "Epoch 583/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0211 - acc: 0.9962 - val_loss: 0.6709 - val_acc: 0.8857\n",
            "Epoch 584/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0216 - acc: 0.9958 - val_loss: 0.6643 - val_acc: 0.8838\n",
            "Epoch 585/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0211 - acc: 0.9958 - val_loss: 0.6665 - val_acc: 0.8862\n",
            "Epoch 586/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0208 - acc: 0.9959 - val_loss: 0.6709 - val_acc: 0.8866\n",
            "Epoch 587/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0212 - acc: 0.9955 - val_loss: 0.6660 - val_acc: 0.8853\n",
            "Epoch 588/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0207 - acc: 0.9964 - val_loss: 0.6742 - val_acc: 0.8861\n",
            "Epoch 589/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0207 - acc: 0.9960 - val_loss: 0.7008 - val_acc: 0.8792\n",
            "Epoch 590/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0210 - acc: 0.9956 - val_loss: 0.6820 - val_acc: 0.8819\n",
            "Epoch 591/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0204 - acc: 0.9963 - val_loss: 0.6817 - val_acc: 0.8846\n",
            "Epoch 592/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0212 - acc: 0.9955 - val_loss: 0.6876 - val_acc: 0.8827\n",
            "Epoch 593/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0202 - acc: 0.9962 - val_loss: 0.6796 - val_acc: 0.8862\n",
            "Epoch 594/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0199 - acc: 0.9963 - val_loss: 0.6739 - val_acc: 0.8869\n",
            "Epoch 595/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0202 - acc: 0.9962 - val_loss: 0.6808 - val_acc: 0.8858\n",
            "Epoch 596/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0205 - acc: 0.9959 - val_loss: 0.6782 - val_acc: 0.8867\n",
            "Epoch 597/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0199 - acc: 0.9960 - val_loss: 0.6748 - val_acc: 0.8856\n",
            "Epoch 598/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0199 - acc: 0.9959 - val_loss: 0.6877 - val_acc: 0.8846\n",
            "Epoch 599/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0195 - acc: 0.9963 - val_loss: 0.6865 - val_acc: 0.8840\n",
            "Epoch 600/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0195 - acc: 0.9966 - val_loss: 0.6856 - val_acc: 0.8847\n",
            "Epoch 601/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0193 - acc: 0.9964 - val_loss: 0.6889 - val_acc: 0.8852\n",
            "Epoch 602/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0192 - acc: 0.9965 - val_loss: 0.6898 - val_acc: 0.8848\n",
            "Epoch 603/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0191 - acc: 0.9963 - val_loss: 0.6991 - val_acc: 0.8857\n",
            "Epoch 604/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0189 - acc: 0.9966 - val_loss: 0.7177 - val_acc: 0.8805\n",
            "Epoch 605/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0197 - acc: 0.9959 - val_loss: 0.7028 - val_acc: 0.8854\n",
            "Epoch 606/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0190 - acc: 0.9964 - val_loss: 0.6981 - val_acc: 0.8848\n",
            "Epoch 607/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0185 - acc: 0.9967 - val_loss: 0.6854 - val_acc: 0.8855\n",
            "Epoch 608/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0188 - acc: 0.9965 - val_loss: 0.7052 - val_acc: 0.8842\n",
            "Epoch 609/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0185 - acc: 0.9967 - val_loss: 0.6906 - val_acc: 0.8866\n",
            "Epoch 610/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0182 - acc: 0.9970 - val_loss: 0.6913 - val_acc: 0.8849\n",
            "Epoch 611/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0180 - acc: 0.9970 - val_loss: 0.6962 - val_acc: 0.8848\n",
            "Epoch 612/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0185 - acc: 0.9966 - val_loss: 0.7007 - val_acc: 0.8834\n",
            "Epoch 613/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0180 - acc: 0.9969 - val_loss: 0.6991 - val_acc: 0.8851\n",
            "Epoch 614/1000\n",
            "48000/48000 [==============================] - 5s 105us/sample - loss: 0.0181 - acc: 0.9966 - val_loss: 0.6899 - val_acc: 0.8864\n",
            "Epoch 615/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.0179 - acc: 0.9967 - val_loss: 0.6979 - val_acc: 0.8834\n",
            "Epoch 616/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0169 - acc: 0.9971 - val_loss: 0.6991 - val_acc: 0.8842\n",
            "Epoch 617/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0177 - acc: 0.9967 - val_loss: 0.7059 - val_acc: 0.8852\n",
            "Epoch 618/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0174 - acc: 0.9969 - val_loss: 0.7011 - val_acc: 0.8857\n",
            "Epoch 619/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0173 - acc: 0.9973 - val_loss: 0.7095 - val_acc: 0.8848\n",
            "Epoch 620/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0172 - acc: 0.9970 - val_loss: 0.7176 - val_acc: 0.8834\n",
            "Epoch 621/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0173 - acc: 0.9966 - val_loss: 0.7047 - val_acc: 0.8859\n",
            "Epoch 622/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0170 - acc: 0.9970 - val_loss: 0.7010 - val_acc: 0.8857\n",
            "Epoch 623/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0173 - acc: 0.9969 - val_loss: 0.7066 - val_acc: 0.8855\n",
            "Epoch 624/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0176 - acc: 0.9967 - val_loss: 0.7200 - val_acc: 0.8828\n",
            "Epoch 625/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0166 - acc: 0.9971 - val_loss: 0.7101 - val_acc: 0.8861\n",
            "Epoch 626/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0162 - acc: 0.9973 - val_loss: 0.7131 - val_acc: 0.8852\n",
            "Epoch 627/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0167 - acc: 0.9974 - val_loss: 0.7074 - val_acc: 0.8861\n",
            "Epoch 628/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0168 - acc: 0.9973 - val_loss: 0.7308 - val_acc: 0.8842\n",
            "Epoch 629/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0161 - acc: 0.9973 - val_loss: 0.7137 - val_acc: 0.8857\n",
            "Epoch 630/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0166 - acc: 0.9975 - val_loss: 0.7073 - val_acc: 0.8873\n",
            "Epoch 631/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0160 - acc: 0.9973 - val_loss: 0.7152 - val_acc: 0.8862\n",
            "Epoch 632/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0161 - acc: 0.9972 - val_loss: 0.7157 - val_acc: 0.8837\n",
            "Epoch 633/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0156 - acc: 0.9975 - val_loss: 0.7247 - val_acc: 0.8827\n",
            "Epoch 634/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0161 - acc: 0.9971 - val_loss: 0.7234 - val_acc: 0.8847\n",
            "Epoch 635/1000\n",
            "48000/48000 [==============================] - 5s 103us/sample - loss: 0.0161 - acc: 0.9971 - val_loss: 0.7262 - val_acc: 0.8825\n",
            "Epoch 636/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0152 - acc: 0.9976 - val_loss: 0.7268 - val_acc: 0.8858\n",
            "Epoch 637/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0160 - acc: 0.9973 - val_loss: 0.7315 - val_acc: 0.8848\n",
            "Epoch 638/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0155 - acc: 0.9977 - val_loss: 0.7409 - val_acc: 0.8844\n",
            "Epoch 639/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0152 - acc: 0.9978 - val_loss: 0.7287 - val_acc: 0.8833\n",
            "Epoch 640/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0151 - acc: 0.9974 - val_loss: 0.7237 - val_acc: 0.8852\n",
            "Epoch 641/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0154 - acc: 0.9977 - val_loss: 0.7261 - val_acc: 0.8865\n",
            "Epoch 642/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0147 - acc: 0.9978 - val_loss: 0.7318 - val_acc: 0.8838\n",
            "Epoch 643/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0147 - acc: 0.9979 - val_loss: 0.7281 - val_acc: 0.8845\n",
            "Epoch 644/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0149 - acc: 0.9974 - val_loss: 0.7301 - val_acc: 0.8852\n",
            "Epoch 645/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0152 - acc: 0.9976 - val_loss: 0.7399 - val_acc: 0.8813\n",
            "Epoch 646/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0145 - acc: 0.9978 - val_loss: 0.7346 - val_acc: 0.8865\n",
            "Epoch 647/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0150 - acc: 0.9976 - val_loss: 0.7301 - val_acc: 0.8869\n",
            "Epoch 648/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0144 - acc: 0.9978 - val_loss: 0.7390 - val_acc: 0.8855\n",
            "Epoch 649/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0142 - acc: 0.9980 - val_loss: 0.7341 - val_acc: 0.8837\n",
            "Epoch 650/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0153 - acc: 0.9976 - val_loss: 0.7381 - val_acc: 0.8838\n",
            "Epoch 651/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0151 - acc: 0.9972 - val_loss: 0.7468 - val_acc: 0.8826\n",
            "Epoch 652/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0139 - acc: 0.9980 - val_loss: 0.7378 - val_acc: 0.8875\n",
            "Epoch 653/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0149 - acc: 0.9974 - val_loss: 0.7404 - val_acc: 0.8858\n",
            "Epoch 654/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0133 - acc: 0.9980 - val_loss: 0.7396 - val_acc: 0.8848\n",
            "Epoch 655/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0139 - acc: 0.9979 - val_loss: 0.7440 - val_acc: 0.8833\n",
            "Epoch 656/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0139 - acc: 0.9979 - val_loss: 0.7402 - val_acc: 0.8859\n",
            "Epoch 657/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0143 - acc: 0.9979 - val_loss: 0.7461 - val_acc: 0.8848\n",
            "Epoch 658/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0141 - acc: 0.9979 - val_loss: 0.7446 - val_acc: 0.8846\n",
            "Epoch 659/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0149 - acc: 0.9975 - val_loss: 0.7429 - val_acc: 0.8845\n",
            "Epoch 660/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0131 - acc: 0.9983 - val_loss: 0.7494 - val_acc: 0.8852\n",
            "Epoch 661/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0134 - acc: 0.9980 - val_loss: 0.7460 - val_acc: 0.8845\n",
            "Epoch 662/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0132 - acc: 0.9981 - val_loss: 0.7561 - val_acc: 0.8842\n",
            "Epoch 663/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0139 - acc: 0.9980 - val_loss: 0.7612 - val_acc: 0.8810\n",
            "Epoch 664/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0130 - acc: 0.9982 - val_loss: 0.7498 - val_acc: 0.8845\n",
            "Epoch 665/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0130 - acc: 0.9982 - val_loss: 0.7459 - val_acc: 0.8864\n",
            "Epoch 666/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0130 - acc: 0.9982 - val_loss: 0.7522 - val_acc: 0.8854\n",
            "Epoch 667/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0128 - acc: 0.9984 - val_loss: 0.7491 - val_acc: 0.8848\n",
            "Epoch 668/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0125 - acc: 0.9982 - val_loss: 0.7579 - val_acc: 0.8852\n",
            "Epoch 669/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0125 - acc: 0.9985 - val_loss: 0.7527 - val_acc: 0.8856\n",
            "Epoch 670/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0127 - acc: 0.9983 - val_loss: 0.7584 - val_acc: 0.8856\n",
            "Epoch 671/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0127 - acc: 0.9981 - val_loss: 0.7579 - val_acc: 0.8848\n",
            "Epoch 672/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0127 - acc: 0.9985 - val_loss: 0.7676 - val_acc: 0.8821\n",
            "Epoch 673/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0124 - acc: 0.9984 - val_loss: 0.7663 - val_acc: 0.8817\n",
            "Epoch 674/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0119 - acc: 0.9987 - val_loss: 0.7570 - val_acc: 0.8861\n",
            "Epoch 675/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0124 - acc: 0.9982 - val_loss: 0.7777 - val_acc: 0.8850\n",
            "Epoch 676/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0121 - acc: 0.9984 - val_loss: 0.7743 - val_acc: 0.8832\n",
            "Epoch 677/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0121 - acc: 0.9983 - val_loss: 0.7669 - val_acc: 0.8823\n",
            "Epoch 678/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0113 - acc: 0.9988 - val_loss: 0.7669 - val_acc: 0.8853\n",
            "Epoch 679/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0123 - acc: 0.9983 - val_loss: 0.7655 - val_acc: 0.8859\n",
            "Epoch 680/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0119 - acc: 0.9985 - val_loss: 0.7683 - val_acc: 0.8804\n",
            "Epoch 681/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0121 - acc: 0.9984 - val_loss: 0.7657 - val_acc: 0.8856\n",
            "Epoch 682/1000\n",
            "48000/48000 [==============================] - 5s 101us/sample - loss: 0.0116 - acc: 0.9985 - val_loss: 0.7718 - val_acc: 0.8855\n",
            "Epoch 683/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0123 - acc: 0.9981 - val_loss: 0.7680 - val_acc: 0.8844\n",
            "Epoch 684/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0112 - acc: 0.9984 - val_loss: 0.7737 - val_acc: 0.8846\n",
            "Epoch 685/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0112 - acc: 0.9987 - val_loss: 0.7678 - val_acc: 0.8870\n",
            "Epoch 686/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0119 - acc: 0.9980 - val_loss: 0.7698 - val_acc: 0.8848\n",
            "Epoch 687/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0111 - acc: 0.9985 - val_loss: 0.7751 - val_acc: 0.8840\n",
            "Epoch 688/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0113 - acc: 0.9986 - val_loss: 0.7784 - val_acc: 0.8857\n",
            "Epoch 689/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0108 - acc: 0.9988 - val_loss: 0.7806 - val_acc: 0.8850\n",
            "Epoch 690/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0119 - acc: 0.9981 - val_loss: 0.7727 - val_acc: 0.8852\n",
            "Epoch 691/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0112 - acc: 0.9985 - val_loss: 0.7932 - val_acc: 0.8802\n",
            "Epoch 692/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0112 - acc: 0.9987 - val_loss: 0.7792 - val_acc: 0.8837\n",
            "Epoch 693/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0107 - acc: 0.9988 - val_loss: 0.7718 - val_acc: 0.8841\n",
            "Epoch 694/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0107 - acc: 0.9987 - val_loss: 0.7838 - val_acc: 0.8851\n",
            "Epoch 695/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0115 - acc: 0.9985 - val_loss: 0.7867 - val_acc: 0.8857\n",
            "Epoch 696/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0107 - acc: 0.9986 - val_loss: 0.7862 - val_acc: 0.8823\n",
            "Epoch 697/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0108 - acc: 0.9987 - val_loss: 0.7862 - val_acc: 0.8852\n",
            "Epoch 698/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0103 - acc: 0.9989 - val_loss: 0.7899 - val_acc: 0.8838\n",
            "Epoch 699/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0108 - acc: 0.9985 - val_loss: 0.7961 - val_acc: 0.8841\n",
            "Epoch 700/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0108 - acc: 0.9985 - val_loss: 0.7824 - val_acc: 0.8858\n",
            "Epoch 701/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0096 - acc: 0.9990 - val_loss: 0.7895 - val_acc: 0.8863\n",
            "Epoch 702/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0110 - acc: 0.9986 - val_loss: 0.7875 - val_acc: 0.8854\n",
            "Epoch 703/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0107 - acc: 0.9986 - val_loss: 0.7872 - val_acc: 0.8854\n",
            "Epoch 704/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0097 - acc: 0.9989 - val_loss: 0.7896 - val_acc: 0.8839\n",
            "Epoch 705/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0105 - acc: 0.9986 - val_loss: 0.7973 - val_acc: 0.8826\n",
            "Epoch 706/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0104 - acc: 0.9987 - val_loss: 0.7974 - val_acc: 0.8833\n",
            "Epoch 707/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0100 - acc: 0.9990 - val_loss: 0.8002 - val_acc: 0.8853\n",
            "Epoch 708/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.7944 - val_acc: 0.8863\n",
            "Epoch 709/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0097 - acc: 0.9989 - val_loss: 0.7881 - val_acc: 0.8842\n",
            "Epoch 710/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0100 - acc: 0.9988 - val_loss: 0.8014 - val_acc: 0.8848\n",
            "Epoch 711/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0105 - acc: 0.9985 - val_loss: 0.8066 - val_acc: 0.8842\n",
            "Epoch 712/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0103 - acc: 0.9984 - val_loss: 0.7901 - val_acc: 0.8860\n",
            "Epoch 713/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0092 - acc: 0.9990 - val_loss: 0.7999 - val_acc: 0.8830\n",
            "Epoch 714/1000\n",
            "48000/48000 [==============================] - 4s 91us/sample - loss: 0.0100 - acc: 0.9988 - val_loss: 0.8146 - val_acc: 0.8832\n",
            "Epoch 715/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0101 - acc: 0.9986 - val_loss: 0.8007 - val_acc: 0.8863\n",
            "Epoch 716/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0093 - acc: 0.9991 - val_loss: 0.7988 - val_acc: 0.8854\n",
            "Epoch 717/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0096 - acc: 0.9986 - val_loss: 0.7986 - val_acc: 0.8861\n",
            "Epoch 718/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0101 - acc: 0.9987 - val_loss: 0.8021 - val_acc: 0.8847\n",
            "Epoch 719/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0097 - acc: 0.9987 - val_loss: 0.8058 - val_acc: 0.8847\n",
            "Epoch 720/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0095 - acc: 0.9990 - val_loss: 0.8202 - val_acc: 0.8826\n",
            "Epoch 721/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0092 - acc: 0.9990 - val_loss: 0.8076 - val_acc: 0.8862\n",
            "Epoch 722/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0095 - acc: 0.9989 - val_loss: 0.8203 - val_acc: 0.8833\n",
            "Epoch 723/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0089 - acc: 0.9990 - val_loss: 0.8126 - val_acc: 0.8833\n",
            "Epoch 724/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0104 - acc: 0.9987 - val_loss: 0.8111 - val_acc: 0.8848\n",
            "Epoch 725/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0087 - acc: 0.9991 - val_loss: 0.8239 - val_acc: 0.8825\n",
            "Epoch 726/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0089 - acc: 0.9989 - val_loss: 0.8143 - val_acc: 0.8838\n",
            "Epoch 727/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0086 - acc: 0.9992 - val_loss: 0.8137 - val_acc: 0.8832\n",
            "Epoch 728/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0094 - acc: 0.9986 - val_loss: 0.8104 - val_acc: 0.8854\n",
            "Epoch 729/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0095 - acc: 0.9987 - val_loss: 0.8249 - val_acc: 0.8827\n",
            "Epoch 730/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0085 - acc: 0.9990 - val_loss: 0.8112 - val_acc: 0.8842\n",
            "Epoch 731/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0084 - acc: 0.9992 - val_loss: 0.8267 - val_acc: 0.8858\n",
            "Epoch 732/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0095 - acc: 0.9988 - val_loss: 0.8275 - val_acc: 0.8833\n",
            "Epoch 733/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0085 - acc: 0.9991 - val_loss: 0.8083 - val_acc: 0.8855\n",
            "Epoch 734/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0087 - acc: 0.9990 - val_loss: 0.8207 - val_acc: 0.8842\n",
            "Epoch 735/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0090 - acc: 0.9990 - val_loss: 0.8200 - val_acc: 0.8848\n",
            "Epoch 736/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0096 - acc: 0.9987 - val_loss: 0.8225 - val_acc: 0.8831\n",
            "Epoch 737/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0074 - acc: 0.9995 - val_loss: 0.8234 - val_acc: 0.8856\n",
            "Epoch 738/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0086 - acc: 0.9992 - val_loss: 0.8409 - val_acc: 0.8829\n",
            "Epoch 739/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0090 - acc: 0.9990 - val_loss: 0.8259 - val_acc: 0.8846\n",
            "Epoch 740/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0093 - acc: 0.9985 - val_loss: 0.8244 - val_acc: 0.8852\n",
            "Epoch 741/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0077 - acc: 0.9992 - val_loss: 0.8251 - val_acc: 0.8848\n",
            "Epoch 742/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0087 - acc: 0.9988 - val_loss: 0.8339 - val_acc: 0.8808\n",
            "Epoch 743/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0085 - acc: 0.9992 - val_loss: 0.8420 - val_acc: 0.8829\n",
            "Epoch 744/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0082 - acc: 0.9991 - val_loss: 0.8399 - val_acc: 0.8832\n",
            "Epoch 745/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0080 - acc: 0.9991 - val_loss: 0.8352 - val_acc: 0.8845\n",
            "Epoch 746/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0078 - acc: 0.9994 - val_loss: 0.8291 - val_acc: 0.8848\n",
            "Epoch 747/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0077 - acc: 0.9992 - val_loss: 0.8403 - val_acc: 0.8849\n",
            "Epoch 748/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0081 - acc: 0.9991 - val_loss: 0.8390 - val_acc: 0.8836\n",
            "Epoch 749/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0074 - acc: 0.9994 - val_loss: 0.8401 - val_acc: 0.8854\n",
            "Epoch 750/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0079 - acc: 0.9990 - val_loss: 0.8342 - val_acc: 0.8831\n",
            "Epoch 751/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0080 - acc: 0.9990 - val_loss: 0.8436 - val_acc: 0.8829\n",
            "Epoch 752/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0074 - acc: 0.9992 - val_loss: 0.8330 - val_acc: 0.8847\n",
            "Epoch 753/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0084 - acc: 0.9988 - val_loss: 0.8410 - val_acc: 0.8845\n",
            "Epoch 754/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0076 - acc: 0.9994 - val_loss: 0.8425 - val_acc: 0.8835\n",
            "Epoch 755/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0072 - acc: 0.9994 - val_loss: 0.8437 - val_acc: 0.8855\n",
            "Epoch 756/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0087 - acc: 0.9987 - val_loss: 0.8391 - val_acc: 0.8867\n",
            "Epoch 757/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0067 - acc: 0.9995 - val_loss: 0.8384 - val_acc: 0.8845\n",
            "Epoch 758/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0081 - acc: 0.9990 - val_loss: 0.8577 - val_acc: 0.8792\n",
            "Epoch 759/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0076 - acc: 0.9993 - val_loss: 0.8560 - val_acc: 0.8832\n",
            "Epoch 760/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0072 - acc: 0.9994 - val_loss: 0.8483 - val_acc: 0.8843\n",
            "Epoch 761/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0073 - acc: 0.9994 - val_loss: 0.8730 - val_acc: 0.8830\n",
            "Epoch 762/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0071 - acc: 0.9993 - val_loss: 0.8553 - val_acc: 0.8824\n",
            "Epoch 763/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0084 - acc: 0.9990 - val_loss: 0.8561 - val_acc: 0.8832\n",
            "Epoch 764/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0063 - acc: 0.9996 - val_loss: 0.8555 - val_acc: 0.8845\n",
            "Epoch 765/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0069 - acc: 0.9994 - val_loss: 0.8747 - val_acc: 0.8808\n",
            "Epoch 766/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0066 - acc: 0.9992 - val_loss: 0.8505 - val_acc: 0.8855\n",
            "Epoch 767/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0072 - acc: 0.9994 - val_loss: 0.8604 - val_acc: 0.8822\n",
            "Epoch 768/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0070 - acc: 0.9994 - val_loss: 0.8690 - val_acc: 0.8843\n",
            "Epoch 769/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0067 - acc: 0.9995 - val_loss: 0.8615 - val_acc: 0.8831\n",
            "Epoch 770/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0070 - acc: 0.9994 - val_loss: 0.8524 - val_acc: 0.8841\n",
            "Epoch 771/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0075 - acc: 0.9991 - val_loss: 0.8630 - val_acc: 0.8839\n",
            "Epoch 772/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0062 - acc: 0.9996 - val_loss: 0.8608 - val_acc: 0.8854\n",
            "Epoch 773/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0065 - acc: 0.9995 - val_loss: 0.8570 - val_acc: 0.8847\n",
            "Epoch 774/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0078 - acc: 0.9988 - val_loss: 0.8613 - val_acc: 0.8835\n",
            "Epoch 775/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0064 - acc: 0.9995 - val_loss: 0.8579 - val_acc: 0.8864\n",
            "Epoch 776/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0071 - acc: 0.9990 - val_loss: 0.8630 - val_acc: 0.8852\n",
            "Epoch 777/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0071 - acc: 0.9991 - val_loss: 0.8628 - val_acc: 0.8842\n",
            "Epoch 778/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0062 - acc: 0.9995 - val_loss: 0.8570 - val_acc: 0.8852\n",
            "Epoch 779/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0101 - acc: 0.9978 - val_loss: 0.9663 - val_acc: 0.8685\n",
            "Epoch 780/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0052 - acc: 0.9997 - val_loss: 0.8731 - val_acc: 0.8851\n",
            "Epoch 781/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0064 - acc: 0.9994 - val_loss: 0.8728 - val_acc: 0.8829\n",
            "Epoch 782/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0065 - acc: 0.9994 - val_loss: 0.8635 - val_acc: 0.8848\n",
            "Epoch 783/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0061 - acc: 0.9995 - val_loss: 0.8712 - val_acc: 0.8827\n",
            "Epoch 784/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0073 - acc: 0.9989 - val_loss: 0.8706 - val_acc: 0.8839\n",
            "Epoch 785/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0061 - acc: 0.9995 - val_loss: 0.8673 - val_acc: 0.8846\n",
            "Epoch 786/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0074 - acc: 0.9990 - val_loss: 0.8820 - val_acc: 0.8832\n",
            "Epoch 787/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0063 - acc: 0.9993 - val_loss: 0.8834 - val_acc: 0.8833\n",
            "Epoch 788/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0056 - acc: 0.9997 - val_loss: 0.8805 - val_acc: 0.8837\n",
            "Epoch 789/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0062 - acc: 0.9995 - val_loss: 0.8789 - val_acc: 0.8850\n",
            "Epoch 790/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0059 - acc: 0.9997 - val_loss: 0.8744 - val_acc: 0.8832\n",
            "Epoch 791/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0063 - acc: 0.9994 - val_loss: 0.8792 - val_acc: 0.8850\n",
            "Epoch 792/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0063 - acc: 0.9994 - val_loss: 0.8819 - val_acc: 0.8843\n",
            "Epoch 793/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0059 - acc: 0.9994 - val_loss: 0.8786 - val_acc: 0.8856\n",
            "Epoch 794/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0062 - acc: 0.9992 - val_loss: 0.8706 - val_acc: 0.8860\n",
            "Epoch 795/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0057 - acc: 0.9995 - val_loss: 0.9056 - val_acc: 0.8799\n",
            "Epoch 796/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0060 - acc: 0.9995 - val_loss: 0.8887 - val_acc: 0.8849\n",
            "Epoch 797/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0060 - acc: 0.9994 - val_loss: 0.8786 - val_acc: 0.8830\n",
            "Epoch 798/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0058 - acc: 0.9995 - val_loss: 0.8757 - val_acc: 0.8855\n",
            "Epoch 799/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0054 - acc: 0.9997 - val_loss: 0.8952 - val_acc: 0.8832\n",
            "Epoch 800/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0056 - acc: 0.9997 - val_loss: 0.8800 - val_acc: 0.8864\n",
            "Epoch 801/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0063 - acc: 0.9990 - val_loss: 0.8853 - val_acc: 0.8833\n",
            "Epoch 802/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0071 - acc: 0.9989 - val_loss: 0.8799 - val_acc: 0.8839\n",
            "Epoch 803/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0055 - acc: 0.9995 - val_loss: 0.8850 - val_acc: 0.8840\n",
            "Epoch 804/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0055 - acc: 0.9996 - val_loss: 0.8852 - val_acc: 0.8850\n",
            "Epoch 805/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0054 - acc: 0.9997 - val_loss: 0.8860 - val_acc: 0.8833\n",
            "Epoch 806/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0081 - acc: 0.9985 - val_loss: 0.8869 - val_acc: 0.8857\n",
            "Epoch 807/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0046 - acc: 0.9997 - val_loss: 0.8942 - val_acc: 0.8857\n",
            "Epoch 808/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0054 - acc: 0.9995 - val_loss: 0.8955 - val_acc: 0.8836\n",
            "Epoch 809/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0050 - acc: 0.9996 - val_loss: 0.9158 - val_acc: 0.8803\n",
            "Epoch 810/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0061 - acc: 0.9992 - val_loss: 0.8928 - val_acc: 0.8862\n",
            "Epoch 811/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0061 - acc: 0.9992 - val_loss: 0.8946 - val_acc: 0.8852\n",
            "Epoch 812/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0056 - acc: 0.9995 - val_loss: 0.8906 - val_acc: 0.8842\n",
            "Epoch 813/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0051 - acc: 0.9996 - val_loss: 0.8949 - val_acc: 0.8857\n",
            "Epoch 814/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0066 - acc: 0.9991 - val_loss: 0.8942 - val_acc: 0.8854\n",
            "Epoch 815/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0055 - acc: 0.9994 - val_loss: 0.8953 - val_acc: 0.8843\n",
            "Epoch 816/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0052 - acc: 0.9994 - val_loss: 0.9055 - val_acc: 0.8842\n",
            "Epoch 817/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0051 - acc: 0.9995 - val_loss: 0.8989 - val_acc: 0.8830\n",
            "Epoch 818/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0064 - acc: 0.9990 - val_loss: 0.8988 - val_acc: 0.8847\n",
            "Epoch 819/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0049 - acc: 0.9995 - val_loss: 0.9189 - val_acc: 0.8802\n",
            "Epoch 820/1000\n",
            "48000/48000 [==============================] - 5s 102us/sample - loss: 0.0057 - acc: 0.9993 - val_loss: 0.9026 - val_acc: 0.8850\n",
            "Epoch 821/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0043 - acc: 0.9998 - val_loss: 0.8999 - val_acc: 0.8849\n",
            "Epoch 822/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0055 - acc: 0.9994 - val_loss: 0.9291 - val_acc: 0.8807\n",
            "Epoch 823/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0052 - acc: 0.9996 - val_loss: 0.9055 - val_acc: 0.8848\n",
            "Epoch 824/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0045 - acc: 0.9997 - val_loss: 0.9063 - val_acc: 0.8832\n",
            "Epoch 825/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0078 - acc: 0.9985 - val_loss: 0.9003 - val_acc: 0.8848\n",
            "Epoch 826/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0035 - acc: 0.9999 - val_loss: 0.9093 - val_acc: 0.8841\n",
            "Epoch 827/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0047 - acc: 0.9996 - val_loss: 0.9059 - val_acc: 0.8855\n",
            "Epoch 828/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0056 - acc: 0.9992 - val_loss: 0.9083 - val_acc: 0.8853\n",
            "Epoch 829/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0048 - acc: 0.9997 - val_loss: 0.9121 - val_acc: 0.8850\n",
            "Epoch 830/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0055 - acc: 0.9991 - val_loss: 0.9136 - val_acc: 0.8857\n",
            "Epoch 831/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0049 - acc: 0.9996 - val_loss: 0.9066 - val_acc: 0.8853\n",
            "Epoch 832/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0074 - acc: 0.9985 - val_loss: 0.9050 - val_acc: 0.8852\n",
            "Epoch 833/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0043 - acc: 0.9997 - val_loss: 0.9105 - val_acc: 0.8863\n",
            "Epoch 834/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0059 - acc: 0.9988 - val_loss: 0.9137 - val_acc: 0.8863\n",
            "Epoch 835/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0039 - acc: 0.9999 - val_loss: 0.9228 - val_acc: 0.8832\n",
            "Epoch 836/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0052 - acc: 0.9992 - val_loss: 0.9189 - val_acc: 0.8848\n",
            "Epoch 837/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0047 - acc: 0.9996 - val_loss: 0.9220 - val_acc: 0.8843\n",
            "Epoch 838/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0044 - acc: 0.9996 - val_loss: 0.9228 - val_acc: 0.8847\n",
            "Epoch 839/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0072 - acc: 0.9986 - val_loss: 0.9138 - val_acc: 0.8863\n",
            "Epoch 840/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0049 - acc: 0.9992 - val_loss: 0.9193 - val_acc: 0.8848\n",
            "Epoch 841/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0047 - acc: 0.9995 - val_loss: 0.9247 - val_acc: 0.8821\n",
            "Epoch 842/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0039 - acc: 0.9999 - val_loss: 0.9137 - val_acc: 0.8856\n",
            "Epoch 843/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0060 - acc: 0.9990 - val_loss: 0.9186 - val_acc: 0.8848\n",
            "Epoch 844/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0056 - acc: 0.9991 - val_loss: 0.9628 - val_acc: 0.8823\n",
            "Epoch 845/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0037 - acc: 0.9998 - val_loss: 0.9191 - val_acc: 0.8848\n",
            "Epoch 846/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0062 - acc: 0.9986 - val_loss: 0.9171 - val_acc: 0.8862\n",
            "Epoch 847/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0050 - acc: 0.9993 - val_loss: 0.9194 - val_acc: 0.8860\n",
            "Epoch 848/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0033 - acc: 1.0000 - val_loss: 0.9200 - val_acc: 0.8853\n",
            "Epoch 849/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0066 - acc: 0.9988 - val_loss: 0.9211 - val_acc: 0.8859\n",
            "Epoch 850/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0056 - acc: 0.9991 - val_loss: 0.9253 - val_acc: 0.8863\n",
            "Epoch 851/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0048 - acc: 0.9993 - val_loss: 0.9214 - val_acc: 0.8859\n",
            "Epoch 852/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0042 - acc: 0.9997 - val_loss: 0.9273 - val_acc: 0.8812\n",
            "Epoch 853/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0046 - acc: 0.9995 - val_loss: 0.9596 - val_acc: 0.8837\n",
            "Epoch 854/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0044 - acc: 0.9995 - val_loss: 0.9258 - val_acc: 0.8853\n",
            "Epoch 855/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0044 - acc: 0.9995 - val_loss: 0.9248 - val_acc: 0.8867\n",
            "Epoch 856/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0042 - acc: 0.9996 - val_loss: 0.9396 - val_acc: 0.8829\n",
            "Epoch 857/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0064 - acc: 0.9988 - val_loss: 0.9276 - val_acc: 0.8850\n",
            "Epoch 858/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0031 - acc: 0.9999 - val_loss: 0.9283 - val_acc: 0.8843\n",
            "Epoch 859/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0056 - acc: 0.9990 - val_loss: 0.9321 - val_acc: 0.8852\n",
            "Epoch 860/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0039 - acc: 0.9997 - val_loss: 0.9318 - val_acc: 0.8853\n",
            "Epoch 861/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0054 - acc: 0.9991 - val_loss: 0.9265 - val_acc: 0.8865\n",
            "Epoch 862/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0054 - acc: 0.9991 - val_loss: 0.9299 - val_acc: 0.8853\n",
            "Epoch 863/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0043 - acc: 0.9993 - val_loss: 0.9912 - val_acc: 0.8767\n",
            "Epoch 864/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0036 - acc: 0.9996 - val_loss: 0.9339 - val_acc: 0.8852\n",
            "Epoch 865/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0040 - acc: 0.9996 - val_loss: 0.9324 - val_acc: 0.8854\n",
            "Epoch 866/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0056 - acc: 0.9992 - val_loss: 0.9285 - val_acc: 0.8863\n",
            "Epoch 867/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0043 - acc: 0.9994 - val_loss: 0.9364 - val_acc: 0.8859\n",
            "Epoch 868/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.9339 - val_acc: 0.8860\n",
            "Epoch 869/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0027 - acc: 0.9999 - val_loss: 0.9905 - val_acc: 0.8777\n",
            "Epoch 870/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0043 - acc: 0.9994 - val_loss: 0.9532 - val_acc: 0.8838\n",
            "Epoch 871/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0073 - acc: 0.9984 - val_loss: 0.9397 - val_acc: 0.8869\n",
            "Epoch 872/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 0.9348 - val_acc: 0.8849\n",
            "Epoch 873/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0035 - acc: 0.9998 - val_loss: 0.9466 - val_acc: 0.8843\n",
            "Epoch 874/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.9412 - val_acc: 0.8853\n",
            "Epoch 875/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.9521 - val_acc: 0.8832\n",
            "Epoch 876/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.9728 - val_acc: 0.8792\n",
            "Epoch 877/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0036 - acc: 0.9995 - val_loss: 0.9421 - val_acc: 0.8838\n",
            "Epoch 878/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0059 - acc: 0.9988 - val_loss: 0.9764 - val_acc: 0.8788\n",
            "Epoch 879/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0032 - acc: 0.9997 - val_loss: 0.9469 - val_acc: 0.8861\n",
            "Epoch 880/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.9499 - val_acc: 0.8861\n",
            "Epoch 881/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 0.9649 - val_acc: 0.8827\n",
            "Epoch 882/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0036 - acc: 0.9998 - val_loss: 0.9492 - val_acc: 0.8852\n",
            "Epoch 883/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0041 - acc: 0.9993 - val_loss: 0.9496 - val_acc: 0.8856\n",
            "Epoch 884/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0050 - acc: 0.9992 - val_loss: 0.9424 - val_acc: 0.8855\n",
            "Epoch 885/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0031 - acc: 0.9999 - val_loss: 0.9439 - val_acc: 0.8853\n",
            "Epoch 886/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0036 - acc: 0.9995 - val_loss: 0.9524 - val_acc: 0.8853\n",
            "Epoch 887/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0041 - acc: 0.9995 - val_loss: 0.9484 - val_acc: 0.8848\n",
            "Epoch 888/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0038 - acc: 0.9995 - val_loss: 0.9567 - val_acc: 0.8844\n",
            "Epoch 889/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0051 - acc: 0.9991 - val_loss: 0.9476 - val_acc: 0.8856\n",
            "Epoch 890/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0048 - acc: 0.9992 - val_loss: 0.9497 - val_acc: 0.8858\n",
            "Epoch 891/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0025 - acc: 0.9999 - val_loss: 0.9620 - val_acc: 0.8845\n",
            "Epoch 892/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0040 - acc: 0.9994 - val_loss: 0.9499 - val_acc: 0.8862\n",
            "Epoch 893/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0038 - acc: 0.9994 - val_loss: 0.9502 - val_acc: 0.8847\n",
            "Epoch 894/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0050 - acc: 0.9990 - val_loss: 0.9466 - val_acc: 0.8863\n",
            "Epoch 895/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0026 - acc: 0.9999 - val_loss: 0.9525 - val_acc: 0.8851\n",
            "Epoch 896/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0047 - acc: 0.9991 - val_loss: 0.9534 - val_acc: 0.8855\n",
            "Epoch 897/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0037 - acc: 0.9994 - val_loss: 0.9708 - val_acc: 0.8829\n",
            "Epoch 898/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0066 - acc: 0.9984 - val_loss: 0.9483 - val_acc: 0.8840\n",
            "Epoch 899/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9554 - val_acc: 0.8865\n",
            "Epoch 900/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.9594 - val_acc: 0.8848\n",
            "Epoch 901/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 0.9593 - val_acc: 0.8859\n",
            "Epoch 902/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0035 - acc: 0.9996 - val_loss: 0.9602 - val_acc: 0.8861\n",
            "Epoch 903/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0035 - acc: 0.9994 - val_loss: 0.9611 - val_acc: 0.8846\n",
            "Epoch 904/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0026 - acc: 0.9999 - val_loss: 0.9688 - val_acc: 0.8817\n",
            "Epoch 905/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0040 - acc: 0.9995 - val_loss: 0.9598 - val_acc: 0.8862\n",
            "Epoch 906/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0045 - acc: 0.9992 - val_loss: 0.9584 - val_acc: 0.8865\n",
            "Epoch 907/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0046 - acc: 0.9989 - val_loss: 0.9793 - val_acc: 0.8869\n",
            "Epoch 908/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.9616 - val_acc: 0.8867\n",
            "Epoch 909/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0040 - acc: 0.9994 - val_loss: 0.9655 - val_acc: 0.8852\n",
            "Epoch 910/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0034 - acc: 0.9995 - val_loss: 0.9671 - val_acc: 0.8858\n",
            "Epoch 911/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0043 - acc: 0.9995 - val_loss: 0.9657 - val_acc: 0.8858\n",
            "Epoch 912/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0047 - acc: 0.9991 - val_loss: 0.9616 - val_acc: 0.8852\n",
            "Epoch 913/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0045 - acc: 0.9991 - val_loss: 0.9834 - val_acc: 0.8818\n",
            "Epoch 914/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 0.9714 - val_acc: 0.8826\n",
            "Epoch 915/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0031 - acc: 0.9998 - val_loss: 0.9609 - val_acc: 0.8860\n",
            "Epoch 916/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0051 - acc: 0.9989 - val_loss: 0.9575 - val_acc: 0.8857\n",
            "Epoch 917/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0042 - acc: 0.9991 - val_loss: 0.9767 - val_acc: 0.8831\n",
            "Epoch 918/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9644 - val_acc: 0.8867\n",
            "Epoch 919/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0049 - acc: 0.9987 - val_loss: 0.9644 - val_acc: 0.8852\n",
            "Epoch 920/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0038 - acc: 0.9993 - val_loss: 0.9880 - val_acc: 0.8812\n",
            "Epoch 921/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0023 - acc: 0.9999 - val_loss: 0.9739 - val_acc: 0.8842\n",
            "Epoch 922/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0068 - acc: 0.9983 - val_loss: 0.9731 - val_acc: 0.8855\n",
            "Epoch 923/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9685 - val_acc: 0.8858\n",
            "Epoch 924/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0042 - acc: 0.9992 - val_loss: 1.0609 - val_acc: 0.8744\n",
            "Epoch 925/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0032 - acc: 0.9995 - val_loss: 0.9729 - val_acc: 0.8848\n",
            "Epoch 926/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0036 - acc: 0.9995 - val_loss: 0.9743 - val_acc: 0.8856\n",
            "Epoch 927/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0054 - acc: 0.9985 - val_loss: 0.9908 - val_acc: 0.8824\n",
            "Epoch 928/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 0.9704 - val_acc: 0.8851\n",
            "Epoch 929/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0029 - acc: 0.9997 - val_loss: 0.9788 - val_acc: 0.8847\n",
            "Epoch 930/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0046 - acc: 0.9991 - val_loss: 0.9782 - val_acc: 0.8859\n",
            "Epoch 931/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9814 - val_acc: 0.8848\n",
            "Epoch 932/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0048 - acc: 0.9989 - val_loss: 0.9800 - val_acc: 0.8858\n",
            "Epoch 933/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 0.9945 - val_acc: 0.8822\n",
            "Epoch 934/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0064 - acc: 0.9986 - val_loss: 0.9819 - val_acc: 0.8843\n",
            "Epoch 935/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0019 - acc: 1.0000 - val_loss: 0.9751 - val_acc: 0.8864\n",
            "Epoch 936/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0033 - acc: 0.9995 - val_loss: 0.9780 - val_acc: 0.8850\n",
            "Epoch 937/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0037 - acc: 0.9992 - val_loss: 0.9814 - val_acc: 0.8863\n",
            "Epoch 938/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0032 - acc: 0.9993 - val_loss: 1.0429 - val_acc: 0.8780\n",
            "Epoch 939/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 0.9837 - val_acc: 0.8843\n",
            "Epoch 940/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0048 - acc: 0.9989 - val_loss: 0.9862 - val_acc: 0.8835\n",
            "Epoch 941/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0017 - acc: 1.0000 - val_loss: 0.9792 - val_acc: 0.8857\n",
            "Epoch 942/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0056 - acc: 0.9986 - val_loss: 0.9806 - val_acc: 0.8867\n",
            "Epoch 943/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 0.9960 - val_acc: 0.8840\n",
            "Epoch 944/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0034 - acc: 0.9995 - val_loss: 1.0025 - val_acc: 0.8849\n",
            "Epoch 945/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0026 - acc: 0.9997 - val_loss: 0.9907 - val_acc: 0.8840\n",
            "Epoch 946/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0058 - acc: 0.9987 - val_loss: 0.9836 - val_acc: 0.8857\n",
            "Epoch 947/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 1.0077 - val_acc: 0.8814\n",
            "Epoch 948/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0056 - acc: 0.9987 - val_loss: 0.9871 - val_acc: 0.8858\n",
            "Epoch 949/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9876 - val_acc: 0.8854\n",
            "Epoch 950/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0043 - acc: 0.9990 - val_loss: 0.9845 - val_acc: 0.8863\n",
            "Epoch 951/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0079 - acc: 0.9977 - val_loss: 1.0379 - val_acc: 0.8808\n",
            "Epoch 952/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 0.9857 - val_acc: 0.8849\n",
            "Epoch 953/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.9899 - val_acc: 0.8857\n",
            "Epoch 954/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.9929 - val_acc: 0.8855\n",
            "Epoch 955/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0053 - acc: 0.9987 - val_loss: 0.9891 - val_acc: 0.8835\n",
            "Epoch 956/1000\n",
            "48000/48000 [==============================] - 5s 99us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.9941 - val_acc: 0.8857\n",
            "Epoch 957/1000\n",
            "48000/48000 [==============================] - 5s 100us/sample - loss: 0.0043 - acc: 0.9991 - val_loss: 0.9879 - val_acc: 0.8851\n",
            "Epoch 958/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9963 - val_acc: 0.8842\n",
            "Epoch 959/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0061 - acc: 0.9985 - val_loss: 0.9874 - val_acc: 0.8866\n",
            "Epoch 960/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0130 - val_acc: 0.8833\n",
            "Epoch 961/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0063 - acc: 0.9985 - val_loss: 0.9924 - val_acc: 0.8857\n",
            "Epoch 962/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 0.9930 - val_acc: 0.8863\n",
            "Epoch 963/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0047 - acc: 0.9992 - val_loss: 1.0000 - val_acc: 0.8831\n",
            "Epoch 964/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 0.9982 - val_acc: 0.8864\n",
            "Epoch 965/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0027 - acc: 0.9997 - val_loss: 0.9928 - val_acc: 0.8864\n",
            "Epoch 966/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0053 - acc: 0.9986 - val_loss: 0.9926 - val_acc: 0.8853\n",
            "Epoch 967/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 0.9960 - val_acc: 0.8866\n",
            "Epoch 968/1000\n",
            "48000/48000 [==============================] - 4s 92us/sample - loss: 0.0052 - acc: 0.9985 - val_loss: 0.9944 - val_acc: 0.8855\n",
            "Epoch 969/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 1.0023 - val_acc: 0.8848\n",
            "Epoch 970/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0049 - acc: 0.9988 - val_loss: 1.0041 - val_acc: 0.8849\n",
            "Epoch 971/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0014 - acc: 1.0000 - val_loss: 1.0026 - val_acc: 0.8852\n",
            "Epoch 972/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0047 - acc: 0.9990 - val_loss: 0.9999 - val_acc: 0.8862\n",
            "Epoch 973/1000\n",
            "48000/48000 [==============================] - 5s 98us/sample - loss: 0.0028 - acc: 0.9995 - val_loss: 1.1102 - val_acc: 0.8745\n",
            "Epoch 974/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0024 - acc: 0.9997 - val_loss: 1.0006 - val_acc: 0.8847\n",
            "Epoch 975/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0041 - acc: 0.9990 - val_loss: 0.9957 - val_acc: 0.8851\n",
            "Epoch 976/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0026 - acc: 0.9995 - val_loss: 1.0605 - val_acc: 0.8783\n",
            "Epoch 977/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0022 - acc: 0.9997 - val_loss: 0.9975 - val_acc: 0.8854\n",
            "Epoch 978/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0039 - acc: 0.9991 - val_loss: 1.0022 - val_acc: 0.8860\n",
            "Epoch 979/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0043 - acc: 0.9989 - val_loss: 1.0010 - val_acc: 0.8846\n",
            "Epoch 980/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0016 - acc: 1.0000 - val_loss: 1.0026 - val_acc: 0.8862\n",
            "Epoch 981/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0050 - acc: 0.9987 - val_loss: 1.0324 - val_acc: 0.8822\n",
            "Epoch 982/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0019 - acc: 0.9998 - val_loss: 0.9996 - val_acc: 0.8855\n",
            "Epoch 983/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0051 - acc: 0.9986 - val_loss: 1.0194 - val_acc: 0.8813\n",
            "Epoch 984/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0059 - val_acc: 0.8855\n",
            "Epoch 985/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0056 - acc: 0.9983 - val_loss: 1.0271 - val_acc: 0.8843\n",
            "Epoch 986/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0015 - acc: 1.0000 - val_loss: 1.0079 - val_acc: 0.8852\n",
            "Epoch 987/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0023 - val_acc: 0.8852\n",
            "Epoch 988/1000\n",
            "48000/48000 [==============================] - 5s 96us/sample - loss: 0.0039 - acc: 0.9993 - val_loss: 1.0053 - val_acc: 0.8855\n",
            "Epoch 989/1000\n",
            "48000/48000 [==============================] - 4s 93us/sample - loss: 0.0070 - acc: 0.9982 - val_loss: 1.0068 - val_acc: 0.8862\n",
            "Epoch 990/1000\n",
            "48000/48000 [==============================] - 5s 97us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0131 - val_acc: 0.8856\n",
            "Epoch 991/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0057 - acc: 0.9986 - val_loss: 1.0040 - val_acc: 0.8840\n",
            "Epoch 992/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 1.0100 - val_acc: 0.8854\n",
            "Epoch 993/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0015 - acc: 0.9999 - val_loss: 1.0688 - val_acc: 0.8789\n",
            "Epoch 994/1000\n",
            "48000/48000 [==============================] - 4s 94us/sample - loss: 0.0031 - acc: 0.9995 - val_loss: 1.0123 - val_acc: 0.8849\n",
            "Epoch 995/1000\n",
            "48000/48000 [==============================] - 5s 95us/sample - loss: 0.0056 - acc: 0.9985 - val_loss: 1.0156 - val_acc: 0.8859\n",
            "Epoch 996/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.0114 - val_acc: 0.8848\n",
            "Epoch 997/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0020 - acc: 0.9999 - val_loss: 1.0244 - val_acc: 0.8832\n",
            "Epoch 998/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0028 - acc: 0.9996 - val_loss: 1.0163 - val_acc: 0.8847\n",
            "Epoch 999/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0083 - acc: 0.9977 - val_loss: 1.0129 - val_acc: 0.8846\n",
            "Epoch 1000/1000\n",
            "48000/48000 [==============================] - 5s 94us/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.0124 - val_acc: 0.8852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI2mnMQ6XfAc",
        "colab_type": "text"
      },
      "source": [
        "## Plotagem de acuracia e perda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y7CRJ18LjuJ",
        "colab_type": "code",
        "outputId": "112bc3f9-c2b8-41e8-bb50-1e6b4e1c2f90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(historico.history['acc'])\n",
        "plt.plot(historico.history['val_acc'])\n",
        "plt.title('Acurácia por epocas')\n",
        "plt.xlabel('epocas')\n",
        "plt.ylabel('acuracia')\n",
        "plt.legend(['treino', 'validacao'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f898abe7fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEuv5QOqUKHx",
        "colab_type": "code",
        "outputId": "b0ee77ba-aab5-4b83-f92b-44fadd4f7e4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(historico.history['loss'])\n",
        "plt.plot(historico.history['val_loss'])\n",
        "plt.title('Perda por epocas')\n",
        "plt.xlabel('epocas')\n",
        "plt.ylabel('perda')\n",
        "plt.legend(['treino', 'validacao'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f898ab35048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n00SglFIOp4lAKaUcThOBUko53P8DgHDhyUKggToAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNivhubkXkL3",
        "colab_type": "text"
      },
      "source": [
        "## Testes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxDSYs66F1xe",
        "colab_type": "code",
        "outputId": "1dd67ee9-ac71-4fe6-da84-c7ef1639fcfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "testes = modelo.predict(imagens_teste)\n",
        "print('Resultado do teste: ', np.argmax(testes[0]))\n",
        "print('Numero da imagem de teste: ' id_teste[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-6afb4eb373eb>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print('Numero da imagem de teste: ' id_teste[0])\u001b[0m\n\u001b[0m                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeTAtOQsGwIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ9Zjs8qH7Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perda_teste, acuracia_teste = modelo.evaluate(imagens_teste, id_teste)\n",
        "print('A perda do teste é de: ', perda_teste)\n",
        "print('A acuracia teste é de: ', acuracia_teste)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdWeqJlYIETg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}